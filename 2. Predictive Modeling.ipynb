{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zhanna Zhanabekova"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IPython Notebook #2 of 3: (Preliminary) Modeling - Regularized Logistic Regression & Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The goal of this project** is to predict the outcome of credit card-related consumer complaints filed with the Consumer Financial Protection Bureau (CFPB). Here, an outcome could result in monetary relief or be settled otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Import useful packages\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import df_processing as p  # contains functions to clean up and transform original CFPB data \n",
    "import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84575, 76)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the cleaned CFPB data\n",
    "directory = os.getcwd() + '/Data/'\n",
    "df = pd.read_pickle(directory+\"df.pickle\")\n",
    "# Print size of the file\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Y', 'Issue_APR or interest rate', 'Issue_Advertising and marketing',\n",
       "       'Issue_Billing disputes', 'Issue_Billing statement',\n",
       "       'Issue_Closing/Cancelling account',\n",
       "       'Issue_Credit card protection / Debt protection',\n",
       "       'Issue_Credit determination', 'Issue_Credit line increase/decrease',\n",
       "       'Issue_Customer service / Customer relations',\n",
       "       'Issue_Delinquent account',\n",
       "       'Issue_Identity theft / Fraud / Embezzlement', 'Issue_Late fee',\n",
       "       'Issue_Other fee', 'Issue_Other groupped', 'Issue_Payoff process',\n",
       "       'Issue_Rewards', 'Issue_Transaction issue',\n",
       "       'Issue_Unsolicited issuance of credit card', 'Is_narrative',\n",
       "       'Company_Amex', 'Company_Bank of America', 'Company_Barclays PLC',\n",
       "       'Company_Capital One', 'Company_Citibank', 'Company_Discover',\n",
       "       'Company_JPMorgan Chase & Co.', 'Company_Synchrony Financial',\n",
       "       'Company_U.S. Bancorp', 'Company_Wells Fargo & Company', 'State_AZ',\n",
       "       'State_CA', 'State_FL', 'State_GA', 'State_IL', 'State_MA', 'State_MD',\n",
       "       'State_MI', 'State_NC', 'State_NJ', 'State_NY', 'State_OH', 'State_PA',\n",
       "       'State_TX', 'State_VA', 'Older American', 'Submitted_Web',\n",
       "       'Submitted_Phone', 'Submitted_Referral', 'Year_2013', 'Year_2014',\n",
       "       'Year_2015', 'Year_2016', 'Year_2017', 'Month_01', 'Month_02',\n",
       "       'Month_03', 'Month_05', 'Month_06', 'Month_07', 'Month_08', 'Month_09',\n",
       "       'Month_10', 'Month_11', 'Month_12', 'unemployment_rate',\n",
       "       'Commutes_to_work_drives_alone', 'Mean_travel_time',\n",
       "       'Median_household_income', 'percent_with_earnings',\n",
       "       'percent_civil_pop_without_health_ins',\n",
       "       'percent_below_poverty_families', 'median_age', 'age_0_19',\n",
       "       'age_65_plus', 'ACS_missing'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recap of variables:**\n",
    "\n",
    "- Outcome Y = 1 if a credit card complaint to the CFPB was resolved with monetary relief, and 0 otherwise.\n",
    "\n",
    "\n",
    "- \"APR or interest rate\" to \"Unsolicited issuance of credit card\" (with prefix \"Issue_\") are 18 recoded Issue dummy variables, based on original 32 Issue categories + the baseline category \"Other.\"\n",
    "\n",
    "\n",
    "- \"Is_narrative\" = 1 when a complaint includes complainant's comments, and 0 otherwise. Potentially, one could apply NLP modeling to these narratives.\n",
    "\n",
    "\n",
    "- \"Amex\" to \"Wells Fargo & Company\" (with prefix \"Company_\") are 10 Company dummies. The excluded category is a higher-level Company grouping.\n",
    "\n",
    "\n",
    "- \"AZ\" to \"VA\" (with prefix \"State_\") are 15 U.S. states. The excluded category is a higher-level grouping of states and territories, including missing values.\n",
    "\n",
    "\n",
    "- \"Older American\" = 1 whenever variable Tags identifies a complainant as such, and 0 otherwise (\"Older American, Servicemember\", \"Servicemember\", or nulls).\n",
    "\n",
    "\n",
    "- \"Consent provided\" = 1 if a complainant agreed to share the contents of his/her complaint publicly, and 0 otherwise.\n",
    "\n",
    "\n",
    "- Three submission method dummy variables - \"Submitted_Web,\" \"Submitted_Phone,\" and \"Submitted_Referral.\" The excluded category is submissions via mail, email, and fax. \n",
    "\n",
    "\n",
    "- The remaining variables include yearly dummies 2013-2017 (the excluded year is 2012), monthly dummies (the excluded month is April), socio-economic variables from the American Community Survey (ACS), and a dummy for zip codes in the CFPB dataset that could not be matched to ACS data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train, validation, and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train, valid, and test files:  (50745, 78) (16915, 78) (16915, 78)\n"
     ]
    }
   ],
   "source": [
    "# Take natural log of ACS's household median income (more on this below)\n",
    "df['ln_median_income'] = np.log(df.Median_household_income)\n",
    "\n",
    "# Add a constant term to df\n",
    "df = sm.add_constant(df)\n",
    "#ref: http://geekwentfreak.com/posts/stats/statsmodels_add_constant\n",
    "\n",
    "#Create train, validation, and test sets \n",
    "train = df.sample(frac=0.6,replace = False, random_state=200)\n",
    "temp = df.drop(train.index)\n",
    "valid = temp.sample(frac=0.5,replace = False, random_state=100)\n",
    "test = temp.drop(valid.index)\n",
    "print(\"Size of train, valid, and test files: \", train.shape, valid.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Unregularized logistic regression using \"as is\" ACS variables (except log-transformed median household income)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use statsmodels.api to fit a logistic regression model to train data. Unlike the sklearn logistic regression package, statsmodels.api allows us to examine the statistical significance of estimated coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.458000\n",
      "         Iterations 8\n"
     ]
    }
   ],
   "source": [
    "# Fit the logistic regression to train data (un-preprocessed)\n",
    "logit_base = sm.Logit(train['Y'], train.drop(['Y', 'Median_household_income'], axis=1)) \n",
    "# include ln(Median household income) instead of Median household income\n",
    "logit_base = logit_base.fit()\n",
    "\n",
    "# Examine the fitted model's parameters\n",
    "#logit_sm_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of coefficients with p-value of 0.05 or less:  44\n",
      "# of coefficients with p-value of 0.01 or less:  38\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>pvalue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Issue_Late fee</th>\n",
       "      <td>2.109</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Issue_Other fee</th>\n",
       "      <td>1.880</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Issue_Billing disputes</th>\n",
       "      <td>1.179</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Company_Citibank</th>\n",
       "      <td>0.960</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Issue_Billing statement</th>\n",
       "      <td>0.957</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Issue_Transaction issue</th>\n",
       "      <td>0.952</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Issue_APR or interest rate</th>\n",
       "      <td>0.941</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Issue_Credit card protection / Debt protection</th>\n",
       "      <td>0.899</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Company_Barclays PLC</th>\n",
       "      <td>0.891</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Company_Synchrony Financial</th>\n",
       "      <td>0.834</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Issue_Payoff process</th>\n",
       "      <td>0.613</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Year_2013</th>\n",
       "      <td>0.498</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Issue_Identity theft / Fraud / Embezzlement</th>\n",
       "      <td>0.407</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Year_2014</th>\n",
       "      <td>0.390</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Issue_Customer service / Customer relations</th>\n",
       "      <td>0.352</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Month_07</th>\n",
       "      <td>0.328</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Company_Capital One</th>\n",
       "      <td>0.321</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Issue_Rewards</th>\n",
       "      <td>0.316</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Month_10</th>\n",
       "      <td>0.302</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Month_08</th>\n",
       "      <td>0.269</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Month_06</th>\n",
       "      <td>0.267</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Submitted_Web</th>\n",
       "      <td>0.254</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Company_Discover</th>\n",
       "      <td>0.252</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Month_09</th>\n",
       "      <td>0.248</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Year_2017</th>\n",
       "      <td>0.247</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Issue_Other groupped</th>\n",
       "      <td>0.230</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Month_11</th>\n",
       "      <td>0.192</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Month_12</th>\n",
       "      <td>0.187</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Company_Amex</th>\n",
       "      <td>0.179</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Year_2016</th>\n",
       "      <td>0.179</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Is_narrative</th>\n",
       "      <td>0.176</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Issue_Closing/Cancelling account</th>\n",
       "      <td>0.174</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Year_2015</th>\n",
       "      <td>0.161</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Older American</th>\n",
       "      <td>0.155</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Month_05</th>\n",
       "      <td>0.136</td>\n",
       "      <td>0.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Submitted_Referral</th>\n",
       "      <td>0.124</td>\n",
       "      <td>0.027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unemployment_rate</th>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State_NY</th>\n",
       "      <td>-0.107</td>\n",
       "      <td>0.039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State_TX</th>\n",
       "      <td>-0.147</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State_MD</th>\n",
       "      <td>-0.164</td>\n",
       "      <td>0.028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Issue_Unsolicited issuance of credit card</th>\n",
       "      <td>-0.290</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Issue_Credit line increase/decrease</th>\n",
       "      <td>-0.937</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>const</th>\n",
       "      <td>-1.764</td>\n",
       "      <td>0.046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Issue_Credit determination</th>\n",
       "      <td>-1.854</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 coef  pvalue\n",
       "Issue_Late fee                                  2.109   0.000\n",
       "Issue_Other fee                                 1.880   0.000\n",
       "Issue_Billing disputes                          1.179   0.000\n",
       "Company_Citibank                                0.960   0.000\n",
       "Issue_Billing statement                         0.957   0.000\n",
       "Issue_Transaction issue                         0.952   0.000\n",
       "Issue_APR or interest rate                      0.941   0.000\n",
       "Issue_Credit card protection / Debt protection  0.899   0.000\n",
       "Company_Barclays PLC                            0.891   0.000\n",
       "Company_Synchrony Financial                     0.834   0.000\n",
       "Issue_Payoff process                            0.613   0.000\n",
       "Year_2013                                       0.498   0.000\n",
       "Issue_Identity theft / Fraud / Embezzlement     0.407   0.000\n",
       "Year_2014                                       0.390   0.000\n",
       "Issue_Customer service / Customer relations     0.352   0.000\n",
       "Month_07                                        0.328   0.000\n",
       "Company_Capital One                             0.321   0.000\n",
       "Issue_Rewards                                   0.316   0.000\n",
       "Month_10                                        0.302   0.000\n",
       "Month_08                                        0.269   0.000\n",
       "Month_06                                        0.267   0.000\n",
       "Submitted_Web                                   0.254   0.000\n",
       "Company_Discover                                0.252   0.000\n",
       "Month_09                                        0.248   0.000\n",
       "Year_2017                                       0.247   0.000\n",
       "Issue_Other groupped                            0.230   0.000\n",
       "Month_11                                        0.192   0.001\n",
       "Month_12                                        0.187   0.002\n",
       "Company_Amex                                    0.179   0.003\n",
       "Year_2016                                       0.179   0.000\n",
       "Is_narrative                                    0.176   0.007\n",
       "Issue_Closing/Cancelling account                0.174   0.007\n",
       "Year_2015                                       0.161   0.000\n",
       "Older American                                  0.155   0.000\n",
       "Month_05                                        0.136   0.024\n",
       "Submitted_Referral                              0.124   0.027\n",
       "unemployment_rate                              -0.015   0.009\n",
       "State_NY                                       -0.107   0.039\n",
       "State_TX                                       -0.147   0.007\n",
       "State_MD                                       -0.164   0.028\n",
       "Issue_Unsolicited issuance of credit card      -0.290   0.010\n",
       "Issue_Credit line increase/decrease            -0.937   0.000\n",
       "const                                          -1.764   0.046\n",
       "Issue_Credit determination                     -1.854   0.000"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyze estimated coefficients significant at least at 5% level\n",
    "pd.options.display.float_format = '{:,.3f}'.format\n",
    "results =   pd.concat([pd.DataFrame(logit_base.params,columns=['coef']),\\\n",
    "            pd.DataFrame(logit_base.pvalues, columns=['pvalue'])], axis=1)\n",
    "print('# of coefficients with p-value of 0.05 or less: ', \n",
    "            results[results.pvalue <= 0.05].shape[0])\n",
    "print('# of coefficients with p-value of 0.01 or less: ', \n",
    "            results[results.pvalue <= 0.01].shape[0])\n",
    "results[results.pvalue <= 0.05].sort_values(by = 'coef', ascending = False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of coefficients with p-value of greater than 0.05:  32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>pvalue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Company_Wells Fargo &amp; Company</th>\n",
       "      <td>0.080</td>\n",
       "      <td>0.256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Month_02</th>\n",
       "      <td>0.062</td>\n",
       "      <td>0.294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State_OH</th>\n",
       "      <td>0.061</td>\n",
       "      <td>0.349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Submitted_Phone</th>\n",
       "      <td>0.041</td>\n",
       "      <td>0.523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State_MI</th>\n",
       "      <td>0.040</td>\n",
       "      <td>0.624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Company_U.S. Bancorp</th>\n",
       "      <td>0.040</td>\n",
       "      <td>0.643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Month_01</th>\n",
       "      <td>0.036</td>\n",
       "      <td>0.550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median_age</th>\n",
       "      <td>0.003</td>\n",
       "      <td>0.654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>percent_civil_pop_without_health_ins</th>\n",
       "      <td>0.002</td>\n",
       "      <td>0.502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean_travel_time</th>\n",
       "      <td>0.002</td>\n",
       "      <td>0.573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>percent_with_earnings</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_0_19</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Commutes_to_work_drives_alone</th>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_65_plus</th>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Issue_Delinquent account</th>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>percent_below_poverty_families</th>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State_CA</th>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Month_03</th>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACS_missing</th>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State_IL</th>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State_VA</th>\n",
       "      <td>-0.028</td>\n",
       "      <td>0.689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Company_JPMorgan Chase &amp; Co.</th>\n",
       "      <td>-0.036</td>\n",
       "      <td>0.508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State_MA</th>\n",
       "      <td>-0.043</td>\n",
       "      <td>0.587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State_FL</th>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State_GA</th>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State_PA</th>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Company_Bank of America</th>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State_NJ</th>\n",
       "      <td>-0.081</td>\n",
       "      <td>0.194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State_AZ</th>\n",
       "      <td>-0.089</td>\n",
       "      <td>0.281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State_NC</th>\n",
       "      <td>-0.092</td>\n",
       "      <td>0.210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ln_median_income</th>\n",
       "      <td>-0.114</td>\n",
       "      <td>0.189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Issue_Advertising and marketing</th>\n",
       "      <td>-0.167</td>\n",
       "      <td>0.063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       coef  pvalue\n",
       "Company_Wells Fargo & Company         0.080   0.256\n",
       "Month_02                              0.062   0.294\n",
       "State_OH                              0.061   0.349\n",
       "Submitted_Phone                       0.041   0.523\n",
       "State_MI                              0.040   0.624\n",
       "Company_U.S. Bancorp                  0.040   0.643\n",
       "Month_01                              0.036   0.550\n",
       "median_age                            0.003   0.654\n",
       "percent_civil_pop_without_health_ins  0.002   0.502\n",
       "Mean_travel_time                      0.002   0.573\n",
       "percent_with_earnings                 0.001   0.776\n",
       "age_0_19                              0.001   0.784\n",
       "Commutes_to_work_drives_alone        -0.001   0.547\n",
       "age_65_plus                          -0.002   0.704\n",
       "Issue_Delinquent account             -0.005   0.949\n",
       "percent_below_poverty_families       -0.006   0.171\n",
       "State_CA                             -0.008   0.840\n",
       "Month_03                             -0.008   0.887\n",
       "ACS_missing                          -0.023   0.700\n",
       "State_IL                             -0.023   0.729\n",
       "State_VA                             -0.028   0.689\n",
       "Company_JPMorgan Chase & Co.         -0.036   0.508\n",
       "State_MA                             -0.043   0.587\n",
       "State_FL                             -0.053   0.280\n",
       "State_GA                             -0.058   0.410\n",
       "State_PA                             -0.065   0.308\n",
       "Company_Bank of America              -0.065   0.245\n",
       "State_NJ                             -0.081   0.194\n",
       "State_AZ                             -0.089   0.281\n",
       "State_NC                             -0.092   0.210\n",
       "ln_median_income                     -0.114   0.189\n",
       "Issue_Advertising and marketing      -0.167   0.063"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at statistically insignificant coefficient estimates (alpha > 0.05)\n",
    "print('# of coefficients with p-value of greater than 0.05: ', \n",
    "            results[results.pvalue > 0.05].shape[0])\n",
    "results[results.pvalue > 0.05].sort_values(by = 'coef', ascending = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha <= 0.05: \n",
      "mean of abs coefficients 0.542, median 0.296\n",
      "\n",
      "alpha > 0.05: \n",
      "mean of abs coefficients 0.042, median 0.038\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Examine the average and median size of abs value of stats significant and insignificant coefficients\n",
    "print('alpha <= 0.05: ')\n",
    "print('mean of abs coefficients {}, median {}'.format(format(abs(results[results.pvalue <= 0.05].coef).mean(), '.3f'), \n",
    "            format(abs(results[results.pvalue <= 0.05].coef).median(), '.3f')))\n",
    "print()\n",
    "\n",
    "print('alpha > 0.05: ')\n",
    "print('mean of abs coefficients {}, median {}'.format(format(abs(results[results.pvalue > 0.05].coef).mean(), '.3f'), \n",
    "            format(abs(results[results.pvalue > 0.05].coef).median(), '.3f')))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.1 Unregularized logit. Comment on estimated coefficients:**\n",
    "\n",
    "To understand what factors are associated with (but don't necessarily cause!) monetary relief in credit card complaints, I examine the **coefficient estimates that are significant at least at the 5% level.** Consistent with my exploratory analysis in IPython Notebook #1, complaints related to fees, billing, transaction issues, and interest rates are more likely to be associated with a **positive** outcome than complaints in the baseline \"Issue_Other\" category and complaints regarding issues such as credit determination, credit line increase/decrease, or unsolicited issuance of credit cards that are more likely to result in non-monetary relief. \n",
    "\n",
    "\n",
    "Similarly, Citibank, Barclays, and Synchrony are more likely to offer monetary relief than the groupped (smaller) companies in the baseline category. The likelihood of a positive outcome was higher in years 2013 and 2014 relative to 2012 (baseline year) and in July relative to April (baseline month). \n",
    "\n",
    "In fact, if you look closely, the ordering of statistically significant coefficients shown above is essentially the same as the ordering of categories of variables 'Issue' and 'Company' by the % of positive outcomes reported in IPython Notebook #1 (marked as Tables 1 and 2). That reflects the nature of regressions with dummy variables where an estimated coefficient captures the incremental effect of a category encoded by the dummy relative to the baseline (reflected in the coefficient on the constant term).\n",
    "\n",
    "The probability of a **negative** outcome is higher in states with higher unemployment rate. Complaints originating from New York, Texas, and Maryland are less likely to be closed with monetary relief than complaints coming from smaller states and territories that I groupped in the baseline category. \n",
    "\n",
    "Finally, note that while **statistically insignificant coefficients** (e.g., on dummies for whether the consent to share info was provided, or whether complaint description was provided) do contribute to predicting the probability of Y=1, the average size of their absolute values is much smaller compared to that of statistically significant coefficients (0.042 vs 0.542, respectively). The majority of ACS variables and state dummies appear to be not as important in predicting the outcome as some of the Issue categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_no_preprocess': {'AUC': '0.713148',\n",
       "  'logloss_train': '0.458000',\n",
       "  'logloss_valid': '0.467460'}}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate AUC (on valid) and log-loss (on train & valid) for the fitted unregularized logit\n",
    "auc_and_logloss_results = {}\n",
    "auc_and_logloss_results = f.store_AUC_and_logloss_results(logit_base, \\\n",
    "            'base_no_preprocess', False, train, valid, \\\n",
    "            ['Y', 'Median_household_income'],auc_and_logloss_results)\n",
    "auc_and_logloss_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_no_preprocess, threshold of 0.6': {'fpr': '0.008', 'fnr': '0.943'},\n",
       " 'base_no_preprocess, threshold of 0.55': {'fpr': '0.012', 'fnr': '0.927'},\n",
       " 'base_no_preprocess, threshold of 0.5': {'fpr': '0.021', 'fnr': '0.898'},\n",
       " 'base_no_preprocess, threshold of 0.45': {'fpr': '0.041', 'fnr': '0.836'},\n",
       " 'base_no_preprocess, threshold of 0.4': {'fpr': '0.070', 'fnr': '0.743'},\n",
       " 'base_no_preprocess, threshold of 0.35': {'fpr': '0.103', 'fnr': '0.661'},\n",
       " 'base_no_preprocess, threshold of 0.3': {'fpr': '0.150', 'fnr': '0.590'},\n",
       " 'base_no_preprocess, threshold of 0.25': {'fpr': '0.237', 'fnr': '0.473'},\n",
       " 'base_no_preprocess, threshold of 0.2': {'fpr': '0.366', 'fnr': '0.327'}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate % of false positives and false negatives on the unregularized logit\n",
    "fpr_and_fnr_results_base = {}\n",
    "for i in  [i/100 for i in range(60,15,-5)]:\n",
    "    fpr_and_fnr_results_base = \\\n",
    "    f.store_fpr_and_fnr_results(logit_base, \\\n",
    "    'base_no_preprocess, threshold of {}'.format(i), False, \\\n",
    "    valid, ['Y', 'Median_household_income'], i, fpr_and_fnr_results_base)\n",
    "fpr_and_fnr_results_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Histograms of Predicted Probabilities when Y=1 and Y=1 using validation set\n",
    "Y_and_Y_hat = f.actual_and_predicted_values(logit_base, valid, False, \\\n",
    "              ['Y', 'Median_household_income'])\n",
    "f.predicted_proba_histograms_by_Y(Y_and_Y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Unregularized logit. Comment on model performance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using training data:**\n",
    "\n",
    "The **log-loss** of unregularized logit is **0.458** (adjusted for N).  The log-loss on validation set is very close, at **0.467**. \n",
    "\n",
    "**Using validation data:**\n",
    "\n",
    "**AUC** is 0.713148.\n",
    "\n",
    "When we set the **decision threshold at 0.5**, the **false positive rate, or Pr(Type I error), or Pr(predict relief when none)** is relatively low at 0.021. But the false negative rate is quite high:\n",
    "\n",
    "**False negative rate, or Pr(Type II error), or Pr(predict no \\$ when compensation granted)** is 0.898. \n",
    "\n",
    "In short, the estimated unregularized logit model favors the negative, majority class. It fails to predict a positive label in the majority of cases with actual monetary relief with the given decision threshold. \n",
    "\n",
    "If we **lower the decision threshold to 0.2**, the false negative rate drops to 0.327 - that is, we are able to identify roughly 67\\% of all complaints with monetary relief correctly. However, we also end up with a lot of false positives - complaints that we label as settled with monetary compensation when, in reality, they were not (FPR = 0.366). \n",
    "\n",
    "Now take a look at **histograms:** \n",
    "\n",
    "- When I examine estimated probabilities for actual Y=0, the patterns looks good, meaning that the majority of predicted probabilities are less than 0.5 and are classified as Y-hat = 0. \n",
    "- For Y=1, again the bulk of estimated probabilities tend to lie **below** the 0.5 threshold, which leads to incorrect predictions of Y-hat=0. \n",
    "- **One solution** could be to lower the decision threshold from 0.5 to something smaller. This would lower the Pr(Type II error), but increase Pr(Type I error). The problem, however, is that with the current set of predictions, for us to minimize Pr(Type II error), the revised threshold would have to tend to zero, which would inflate Pr(Type I error) close to 1. The model will essentially continue being subpar.\n",
    "- So, **ideally**, we want to improve our model so that the first histogram would be skewed more to the right instead of the left - i.e., give higher probability estimates when Y=1 for the data points that are currently ranked below the decision threshold of 0.5.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I continue experimenting with various model enhancements to see if I can impprove its predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Comment on feature pre-processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I log-transformed one of the ACS (Census) variables above: median household income. Log-transformation 'squeezes' the distribution of an otherwise skewed variable, susceptible to the influence of outliers. And, unlike in decision trees, outliers can really distort coefficient estimates in OLS and logit regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot histograms of median household income and its log transformed version\n",
    "\n",
    "# Median Household Income\n",
    "plt.hist(df['Median_household_income'])\n",
    "plt.title('Median household income (raw)')\n",
    "plt.show()\n",
    "\n",
    "# Ln(Median Household Income)\n",
    "plt.hist(df['ln_median_income'])\n",
    "plt.title('Log-transformed median household income')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of feature pre-processing is to ensure that the scale of variables included in the regression model is more or less the same. This helps minimize the risk of scenarios where estimation is dominated by a few large features. More on this here: https://scikit-learn.org/stable/modules/preprocessing.html. \n",
    "\n",
    "\n",
    "Below, I only pre-process ACS (Census) variables and do not demean and rescale dummy variables. First, dummy variables are already constrained to be either 0 or 1. Second, I also care about the interpretation of estimated coefficients on dummies, though this is not the regular practice or objective in a lot of machine learning projects. \n",
    "\n",
    "Moreover, as we will see below, preprocessing does not really change the fundamental results for as long as I do not use any higher-order polynomials. It changes coefficient estimates on the preprocessed variables and the constant term but does not affect the coefficients of unscaled dummy variables, predicted probabilities, and AUC. In addition, the logit results reported above show that coefficient estimates on un-preprocessed ACS variables are small and mostly statistically insignificant what assuages any possible concerns regarding misleading, scale-driven influence of continuous raw ACS variables relative to dummies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define ACS variables\n",
    "ACS_vars = ['unemployment_rate', 'Commutes_to_work_drives_alone',\n",
    "       'Mean_travel_time', 'Median_household_income', 'percent_with_earnings',\n",
    "       'percent_civil_pop_without_health_ins',\n",
    "       'percent_below_poverty_families', 'median_age', 'age_0_19',\n",
    "       'age_65_plus', 'ln_median_income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>unemployment_rate</th>\n",
       "      <td>84,575.000</td>\n",
       "      <td>7.942</td>\n",
       "      <td>3.344</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.000</td>\n",
       "      <td>7.942</td>\n",
       "      <td>8.600</td>\n",
       "      <td>100.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Commutes_to_work_drives_alone</th>\n",
       "      <td>84,575.000</td>\n",
       "      <td>73.483</td>\n",
       "      <td>15.188</td>\n",
       "      <td>0.000</td>\n",
       "      <td>73.483</td>\n",
       "      <td>75.400</td>\n",
       "      <td>81.900</td>\n",
       "      <td>100.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean_travel_time</th>\n",
       "      <td>84,575.000</td>\n",
       "      <td>27.106</td>\n",
       "      <td>5.528</td>\n",
       "      <td>6.300</td>\n",
       "      <td>23.700</td>\n",
       "      <td>27.106</td>\n",
       "      <td>29.400</td>\n",
       "      <td>113.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Median_household_income</th>\n",
       "      <td>84,575.000</td>\n",
       "      <td>66,571.234</td>\n",
       "      <td>23,730.800</td>\n",
       "      <td>3,479.000</td>\n",
       "      <td>51,079.000</td>\n",
       "      <td>66,571.234</td>\n",
       "      <td>73,856.000</td>\n",
       "      <td>236,500.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>percent_with_earnings</th>\n",
       "      <td>84,575.000</td>\n",
       "      <td>79.200</td>\n",
       "      <td>7.387</td>\n",
       "      <td>0.000</td>\n",
       "      <td>77.300</td>\n",
       "      <td>79.200</td>\n",
       "      <td>83.200</td>\n",
       "      <td>100.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>percent_civil_pop_without_health_ins</th>\n",
       "      <td>84,575.000</td>\n",
       "      <td>11.434</td>\n",
       "      <td>5.738</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.700</td>\n",
       "      <td>11.434</td>\n",
       "      <td>13.100</td>\n",
       "      <td>100.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>percent_below_poverty_families</th>\n",
       "      <td>84,575.000</td>\n",
       "      <td>9.948</td>\n",
       "      <td>6.824</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.300</td>\n",
       "      <td>9.948</td>\n",
       "      <td>11.100</td>\n",
       "      <td>100.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median_age</th>\n",
       "      <td>84,575.000</td>\n",
       "      <td>38.949</td>\n",
       "      <td>5.493</td>\n",
       "      <td>15.500</td>\n",
       "      <td>36.000</td>\n",
       "      <td>38.949</td>\n",
       "      <td>41.100</td>\n",
       "      <td>91.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_0_19</th>\n",
       "      <td>84,575.000</td>\n",
       "      <td>24.591</td>\n",
       "      <td>5.047</td>\n",
       "      <td>0.000</td>\n",
       "      <td>23.000</td>\n",
       "      <td>24.591</td>\n",
       "      <td>26.900</td>\n",
       "      <td>91.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_65_plus</th>\n",
       "      <td>84,575.000</td>\n",
       "      <td>14.565</td>\n",
       "      <td>5.721</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.600</td>\n",
       "      <td>14.565</td>\n",
       "      <td>15.800</td>\n",
       "      <td>100.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ln_median_income</th>\n",
       "      <td>84,575.000</td>\n",
       "      <td>11.046</td>\n",
       "      <td>0.350</td>\n",
       "      <td>8.155</td>\n",
       "      <td>10.841</td>\n",
       "      <td>11.106</td>\n",
       "      <td>11.210</td>\n",
       "      <td>12.374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          count       mean        std  \\\n",
       "unemployment_rate                    84,575.000      7.942      3.344   \n",
       "Commutes_to_work_drives_alone        84,575.000     73.483     15.188   \n",
       "Mean_travel_time                     84,575.000     27.106      5.528   \n",
       "Median_household_income              84,575.000 66,571.234 23,730.800   \n",
       "percent_with_earnings                84,575.000     79.200      7.387   \n",
       "percent_civil_pop_without_health_ins 84,575.000     11.434      5.738   \n",
       "percent_below_poverty_families       84,575.000      9.948      6.824   \n",
       "median_age                           84,575.000     38.949      5.493   \n",
       "age_0_19                             84,575.000     24.591      5.047   \n",
       "age_65_plus                          84,575.000     14.565      5.721   \n",
       "ln_median_income                     84,575.000     11.046      0.350   \n",
       "\n",
       "                                           min        25%        50%  \\\n",
       "unemployment_rate                        0.000      6.000      7.942   \n",
       "Commutes_to_work_drives_alone            0.000     73.483     75.400   \n",
       "Mean_travel_time                         6.300     23.700     27.106   \n",
       "Median_household_income              3,479.000 51,079.000 66,571.234   \n",
       "percent_with_earnings                    0.000     77.300     79.200   \n",
       "percent_civil_pop_without_health_ins     0.000      7.700     11.434   \n",
       "percent_below_poverty_families           0.000      5.300      9.948   \n",
       "median_age                              15.500     36.000     38.949   \n",
       "age_0_19                                 0.000     23.000     24.591   \n",
       "age_65_plus                              0.000     11.600     14.565   \n",
       "ln_median_income                         8.155     10.841     11.106   \n",
       "\n",
       "                                            75%         max  \n",
       "unemployment_rate                         8.600     100.000  \n",
       "Commutes_to_work_drives_alone            81.900     100.000  \n",
       "Mean_travel_time                         29.400     113.500  \n",
       "Median_household_income              73,856.000 236,500.000  \n",
       "percent_with_earnings                    83.200     100.000  \n",
       "percent_civil_pop_without_health_ins     13.100     100.000  \n",
       "percent_below_poverty_families           11.100     100.000  \n",
       "median_age                               41.100      91.800  \n",
       "age_0_19                                 26.900      91.900  \n",
       "age_65_plus                              15.800     100.100  \n",
       "ln_median_income                         11.210      12.374  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary of ACS variables\n",
    "df[ACS_vars].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate the scaler = means and standard deviations of ACS var in train data\n",
    "scaler = preprocessing.StandardScaler().fit(train[ACS_vars])\n",
    "\n",
    "#scaler.mean_     #estimated means\n",
    "#scaler.scale_   #estimated standard deviations\n",
    "\n",
    "#Preprocess train, valid, and test data:\n",
    "train_census_preprocessed = train.copy()\n",
    "valid_census_preprocessed = valid.copy()\n",
    "test_census_preprocessed = test.copy()\n",
    "\n",
    "for i, var in enumerate(ACS_vars):\n",
    "\n",
    "    train_census_preprocessed[var] =\\\n",
    "        (train_census_preprocessed[var] - scaler.mean_[i])/scaler.scale_[i]  \n",
    "    valid_census_preprocessed[var] = \\ \n",
    "        (valid_census_preprocessed[var] - scaler.mean_[i])/scaler.scale_[i]  \n",
    "    test_census_preprocessed[var] = \\\n",
    "        (test_census_preprocessed[var] - scaler.mean_[i])/scaler.scale_[i]  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Unregularized logistic regression using preprocessed ACS variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.458000\n",
      "         Iterations 8\n"
     ]
    }
   ],
   "source": [
    "# Fit the logistic regression to preprocessed train data\n",
    "logit_preprocess_unr = sm.Logit(train_census_preprocessed['Y'], train_census_preprocessed.drop(['Y', 'Median_household_income'], axis=1))\n",
    "logit_preprocess_unr = logit_preprocess_unr.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_no_preprocess': {'AUC': '0.713148',\n",
       "  'logloss_train': '0.458000',\n",
       "  'logloss_valid': '0.467460'},\n",
       " 'base_2_preprocess': {'AUC': '0.713148',\n",
       "  'logloss_train': '0.458000',\n",
       "  'logloss_valid': '0.467460'}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate AUC (on valid) and log-loss (on train & valid) for fitted unregularized logit with preprocessed ACS data\n",
    "auc_and_logloss_results =\\\n",
    "    f.store_AUC_and_logloss_results(logit_preprocess_unr, 'base_2_preprocess', False, \\\n",
    "    train_census_preprocessed, valid_census_preprocessed, ['Y', 'Median_household_income'],auc_and_logloss_results)\n",
    "auc_and_logloss_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_2_preprocess, threshold of 0.5': {'fpr': '0.021', 'fnr': '0.898'}}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate % of false positives and false negatives on the unregularized logit with pre-processed ACS vars\n",
    "fpr_and_fnr_results = {}\n",
    "fpr_and_fnr_results = \\\n",
    "    f.store_fpr_and_fnr_results(logit_preprocess_unr, \\\n",
    "    'base_2_preprocess, threshold of {}'.format(0.5), False, \\\n",
    "    valid_census_preprocessed, ['Y', 'Median_household_income'], 0.5, fpr_and_fnr_results)\n",
    "fpr_and_fnr_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in my model, preprocessing affects the estimated coefficients on the ACS (preprocessed) variables and the constant term only. Other coefficients and AUC remain the same. How pre-processing is 'un-done' in this particular case is shown below:\n",
    "\n",
    "\n",
    "- **Original equation (un-preprocessed train data):** \n",
    "\n",
    "\\begin{equation*}  \n",
    "y = a+bX + cZ \n",
    "\\end{equation*}\n",
    "\n",
    "- **Preprocess X (an ACS variable) and leave dummy Z \"as is\"**\n",
    "\n",
    "\\begin{equation}  \n",
    "\\hat{X} = \\frac{X - \\mu}{\\sigma} \n",
    "\\end{equation}\n",
    "\n",
    "- **Updated equation (preprocessed train data):**\n",
    "\n",
    "\\begin{equation}  \n",
    "y = \\alpha+\\beta \\hat{X} + \\gamma Z  =  \\alpha+\\beta (\\frac{X - \\mu}{\\sigma}) + \\gamma Z = \n",
    "(\\alpha - \\beta \\frac{\\mu}{\\sigma}) +\\frac{\\beta}{\\sigma} X + \\gamma Z\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "- **Hence, the relationship between unpreprocessed and preprocessed coefficients is as follows:**\n",
    "\n",
    "\\begin{equation}  \n",
    "a = \\alpha - \\beta \\frac{\\mu}{\\sigma}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{equation}  \n",
    "b = \\frac{\\mu}{\\sigma}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{equation}  \n",
    "c = \\gamma\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In regressions below I continue using preprocessed train data for consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. L2 (Ridge) Regularized logistic regression using preprocessed ACS variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I introduce Ridge regularization to see if model performance can be improved on the validation set as measured by AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 regularized logit for several hyperparameter values\n",
    "for i in range(6,-7, -1):\n",
    "    logreg = LogisticRegression(C=10**i, fit_intercept = False, solver='liblinear')\n",
    "    logreg = logreg.fit(train_census_preprocessed.drop(['Y', 'Median_household_income'], axis=1), \n",
    "               train_census_preprocessed['Y'])\n",
    "    \n",
    "    # Calculate AUC (on valid) and log-loss (on train & valid) \n",
    "    auc_and_logloss_results = \\\n",
    "        f.store_AUC_and_logloss_results(logreg, \\\n",
    "        'L2 C={} {}'.format(10**i, (7-len(str(10**i)))*\" \"), True, \\\n",
    "        train_census_preprocessed, valid_census_preprocessed, \\\n",
    "        ['Y', 'Median_household_income'],auc_and_logloss_results)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUC</th>\n",
       "      <th>logloss_train</th>\n",
       "      <th>logloss_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>base_no_preprocess</th>\n",
       "      <td>0.713148</td>\n",
       "      <td>0.458000</td>\n",
       "      <td>0.467460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_2_preprocess</th>\n",
       "      <td>0.713148</td>\n",
       "      <td>0.458000</td>\n",
       "      <td>0.467460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=1000000</th>\n",
       "      <td>0.713147</td>\n",
       "      <td>0.458000</td>\n",
       "      <td>0.467460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=100000</th>\n",
       "      <td>0.713146</td>\n",
       "      <td>0.458000</td>\n",
       "      <td>0.467461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=10000</th>\n",
       "      <td>0.713147</td>\n",
       "      <td>0.458000</td>\n",
       "      <td>0.467460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=1000</th>\n",
       "      <td>0.713146</td>\n",
       "      <td>0.458000</td>\n",
       "      <td>0.467461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=100</th>\n",
       "      <td>0.713145</td>\n",
       "      <td>0.458000</td>\n",
       "      <td>0.467462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=10</th>\n",
       "      <td>0.713140</td>\n",
       "      <td>0.458000</td>\n",
       "      <td>0.467462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=1</th>\n",
       "      <td>0.713050</td>\n",
       "      <td>0.458005</td>\n",
       "      <td>0.467482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=0.1</th>\n",
       "      <td>0.712078</td>\n",
       "      <td>0.458313</td>\n",
       "      <td>0.467867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=0.01</th>\n",
       "      <td>0.704437</td>\n",
       "      <td>0.463651</td>\n",
       "      <td>0.472759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=0.001</th>\n",
       "      <td>0.675745</td>\n",
       "      <td>0.487834</td>\n",
       "      <td>0.494544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=0.0001</th>\n",
       "      <td>0.535649</td>\n",
       "      <td>0.532173</td>\n",
       "      <td>0.535842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=1e-05</th>\n",
       "      <td>0.486978</td>\n",
       "      <td>0.631810</td>\n",
       "      <td>0.632692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=1e-06</th>\n",
       "      <td>0.482504</td>\n",
       "      <td>0.684789</td>\n",
       "      <td>0.684894</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         AUC logloss_train logloss_valid\n",
       "base_no_preprocess  0.713148      0.458000      0.467460\n",
       "base_2_preprocess   0.713148      0.458000      0.467460\n",
       "L2 C=1000000        0.713147      0.458000      0.467460\n",
       "L2 C=100000         0.713146      0.458000      0.467461\n",
       "L2 C=10000          0.713147      0.458000      0.467460\n",
       "L2 C=1000           0.713146      0.458000      0.467461\n",
       "L2 C=100            0.713145      0.458000      0.467462\n",
       "L2 C=10             0.713140      0.458000      0.467462\n",
       "L2 C=1              0.713050      0.458005      0.467482\n",
       "L2 C=0.1            0.712078      0.458313      0.467867\n",
       "L2 C=0.01           0.704437      0.463651      0.472759\n",
       "L2 C=0.001          0.675745      0.487834      0.494544\n",
       "L2 C=0.0001         0.535649      0.532173      0.535842\n",
       "L2 C=1e-05          0.486978      0.631810      0.632692\n",
       "L2 C=1e-06          0.482504      0.684789      0.684894"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view_results = pd.DataFrame(auc_and_logloss_results).T\n",
    "view_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUC</th>\n",
       "      <th>logloss_train</th>\n",
       "      <th>logloss_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>base_no_preprocess</th>\n",
       "      <td>0.713148</td>\n",
       "      <td>0.458000</td>\n",
       "      <td>0.467460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_2_preprocess</th>\n",
       "      <td>0.713148</td>\n",
       "      <td>0.458000</td>\n",
       "      <td>0.467460</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         AUC logloss_train logloss_valid\n",
       "base_no_preprocess  0.713148      0.458000      0.467460\n",
       "base_2_preprocess   0.713148      0.458000      0.467460"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the model that gives the highest AUC\n",
    "view_results[view_results.AUC == view_results.AUC.max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results of introducing regularization:**\n",
    "\n",
    "Despite trying several Ridge regularization hyperparameters, it is the baseline model that gives the highest AUC on the validation set. Below, I repeat the same exercise with L1 penalty function and obtain similar results. In short, it looks like regularization is not helping us with making the model developed on the train set more generalizable. **It could be that the original, unregularized model is already simple enough, overfitting is not a major concern, and hence regularization mostly just adds some noise without improving model performance.**\n",
    "\n",
    "**Remember that the baseline model was trained on an already transformed dataset where I tried to minimize the influence of noise by groupping smaller categories together (e.g., went from 33 Issue categories to only 19, from 400+ companies to 11, etc.).**\n",
    "\n",
    "In addition, it could be that there is not sufficient information in the data to understand how to separate classes into Y=1 and Y=0, and the original unregularized regression would be simple enough capturing only basic patterns in data, as suggested by the histograms of estimated probabilities for Y=1 vs Y=0 above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. L1 (Lasso) Regularized logistic regression using preprocessed ACS variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 regularized logit for several hyperparameter values\n",
    "for i in range(6,-7, -1):\n",
    "    logreg = LogisticRegression(penalty='l1', C=10**i, fit_intercept = False, solver='liblinear')\n",
    "    logreg.fit(train_census_preprocessed.drop(['Y', 'Median_household_income'], axis=1), train_census_preprocessed['Y'])\n",
    "   \n",
    "    # Calculate AUC (on valid) and log-loss (on train & valid) \n",
    "    auc_and_logloss_results = \\\n",
    "        f.store_AUC_and_logloss_results(logreg, \\\n",
    "            'L1 C0={} {}'.format(10**i, (7-len(str(10**i)))*\" \"), True, \\\n",
    "            train_census_preprocessed, valid_census_preprocessed, \\\n",
    "            ['Y', 'Median_household_income'],auc_and_logloss_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUC</th>\n",
       "      <th>logloss_train</th>\n",
       "      <th>logloss_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>base_no_preprocess</th>\n",
       "      <td>0.713148</td>\n",
       "      <td>0.458000</td>\n",
       "      <td>0.467460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_2_preprocess</th>\n",
       "      <td>0.713148</td>\n",
       "      <td>0.458000</td>\n",
       "      <td>0.467460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=1000000</th>\n",
       "      <td>0.713147</td>\n",
       "      <td>0.458000</td>\n",
       "      <td>0.467460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=100000</th>\n",
       "      <td>0.713146</td>\n",
       "      <td>0.458000</td>\n",
       "      <td>0.467461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=10000</th>\n",
       "      <td>0.713147</td>\n",
       "      <td>0.458000</td>\n",
       "      <td>0.467460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=1000</th>\n",
       "      <td>0.713146</td>\n",
       "      <td>0.458000</td>\n",
       "      <td>0.467461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=100</th>\n",
       "      <td>0.713145</td>\n",
       "      <td>0.458000</td>\n",
       "      <td>0.467462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=10</th>\n",
       "      <td>0.713140</td>\n",
       "      <td>0.458000</td>\n",
       "      <td>0.467462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=1</th>\n",
       "      <td>0.713050</td>\n",
       "      <td>0.458005</td>\n",
       "      <td>0.467482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=0.1</th>\n",
       "      <td>0.712078</td>\n",
       "      <td>0.458313</td>\n",
       "      <td>0.467867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=0.01</th>\n",
       "      <td>0.704437</td>\n",
       "      <td>0.463651</td>\n",
       "      <td>0.472759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=0.001</th>\n",
       "      <td>0.675745</td>\n",
       "      <td>0.487834</td>\n",
       "      <td>0.494544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=0.0001</th>\n",
       "      <td>0.535649</td>\n",
       "      <td>0.532173</td>\n",
       "      <td>0.535842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=1e-05</th>\n",
       "      <td>0.486978</td>\n",
       "      <td>0.631810</td>\n",
       "      <td>0.632692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=1e-06</th>\n",
       "      <td>0.482504</td>\n",
       "      <td>0.684789</td>\n",
       "      <td>0.684894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L1 C0=1000000</th>\n",
       "      <td>0.713145</td>\n",
       "      <td>0.458000</td>\n",
       "      <td>0.467462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L1 C0=100000</th>\n",
       "      <td>0.713145</td>\n",
       "      <td>0.458000</td>\n",
       "      <td>0.467462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L1 C0=10000</th>\n",
       "      <td>0.713145</td>\n",
       "      <td>0.458000</td>\n",
       "      <td>0.467462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L1 C0=1000</th>\n",
       "      <td>0.713146</td>\n",
       "      <td>0.458000</td>\n",
       "      <td>0.467462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L1 C0=100</th>\n",
       "      <td>0.713145</td>\n",
       "      <td>0.458000</td>\n",
       "      <td>0.467462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L1 C0=10</th>\n",
       "      <td>0.713138</td>\n",
       "      <td>0.458000</td>\n",
       "      <td>0.467462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L1 C0=1</th>\n",
       "      <td>0.713065</td>\n",
       "      <td>0.458009</td>\n",
       "      <td>0.467475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L1 C0=0.1</th>\n",
       "      <td>0.711620</td>\n",
       "      <td>0.458512</td>\n",
       "      <td>0.468075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L1 C0=0.01</th>\n",
       "      <td>0.696594</td>\n",
       "      <td>0.467747</td>\n",
       "      <td>0.477038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L1 C0=0.001</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.513265</td>\n",
       "      <td>0.519024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L1 C0=0.0001</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.600017</td>\n",
       "      <td>0.601822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L1 C0=1e-05</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L1 C0=1e-06</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         AUC logloss_train logloss_valid\n",
       "base_no_preprocess  0.713148      0.458000      0.467460\n",
       "base_2_preprocess   0.713148      0.458000      0.467460\n",
       "L2 C=1000000        0.713147      0.458000      0.467460\n",
       "L2 C=100000         0.713146      0.458000      0.467461\n",
       "L2 C=10000          0.713147      0.458000      0.467460\n",
       "L2 C=1000           0.713146      0.458000      0.467461\n",
       "L2 C=100            0.713145      0.458000      0.467462\n",
       "L2 C=10             0.713140      0.458000      0.467462\n",
       "L2 C=1              0.713050      0.458005      0.467482\n",
       "L2 C=0.1            0.712078      0.458313      0.467867\n",
       "L2 C=0.01           0.704437      0.463651      0.472759\n",
       "L2 C=0.001          0.675745      0.487834      0.494544\n",
       "L2 C=0.0001         0.535649      0.532173      0.535842\n",
       "L2 C=1e-05          0.486978      0.631810      0.632692\n",
       "L2 C=1e-06          0.482504      0.684789      0.684894\n",
       "L1 C0=1000000       0.713145      0.458000      0.467462\n",
       "L1 C0=100000        0.713145      0.458000      0.467462\n",
       "L1 C0=10000         0.713145      0.458000      0.467462\n",
       "L1 C0=1000          0.713146      0.458000      0.467462\n",
       "L1 C0=100           0.713145      0.458000      0.467462\n",
       "L1 C0=10            0.713138      0.458000      0.467462\n",
       "L1 C0=1             0.713065      0.458009      0.467475\n",
       "L1 C0=0.1           0.711620      0.458512      0.468075\n",
       "L1 C0=0.01          0.696594      0.467747      0.477038\n",
       "L1 C0=0.001         0.500000      0.513265      0.519024\n",
       "L1 C0=0.0001        0.500000      0.600017      0.601822\n",
       "L1 C0=1e-05         0.500000      0.693147      0.693147\n",
       "L1 C0=1e-06         0.500000      0.693147      0.693147"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view_results = pd.DataFrame(auc_and_logloss_results).T\n",
    "view_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUC</th>\n",
       "      <th>logloss_train</th>\n",
       "      <th>logloss_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>base_no_preprocess</th>\n",
       "      <td>0.713148</td>\n",
       "      <td>0.458000</td>\n",
       "      <td>0.467460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_2_preprocess</th>\n",
       "      <td>0.713148</td>\n",
       "      <td>0.458000</td>\n",
       "      <td>0.467460</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         AUC logloss_train logloss_valid\n",
       "base_no_preprocess  0.713148      0.458000      0.467460\n",
       "base_2_preprocess   0.713148      0.458000      0.467460"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the model that gives the highest AUC\n",
    "view_results[view_results.AUC == view_results.AUC.max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Adding complexity: Retain the richness of categorical variables in the CFBP set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I want to see if my baseline model can be improved upon if I use the original set of categories/labels in the CFPB variables (e.g., the original set of all 33 Issue categories, 400+ companies, etc.). So, I go back to the downloaded CFPB data, skip the recoding step, and keep all original categories instead. I continue excluding the baseline categories. I also keep the Timely response variable and year 2011 observations that I previously dropped. I end up with a much larger set of dummy variables from the original set of CFPB features (N=555 vs N=76).\n",
    "\n",
    "**Aside note:** If I use statsmodels.api to train an unregularized (or any) logit model, I run into the problem that I initially mentioned in the IPython Notebook #1. Some dummy variables are too sparse: they contain only a few values of 1. This causes problems during estimation as the variance-covariance matrix becomes singular (non-invertible). \n",
    "In sklearn's logistic regression package, I probably do not run into this problem because of the way the objective function is defined there: it always comes with a penalty term. To mimic the unregularized regression using sklearn, we need to set a very high value for parameter C (e.g., 1e30). \n",
    "\n",
    "When I tried using statsmodels.api, after a few trials and errors, I was able to run a logistic regression by dropping dummies with fewer than 300 values of 1. Number of features is 131 vs 76 in the original set of models above. The results are very similar to the ones obtained below with sklearn and are not reported here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of original file:  (738832, 18)\n",
      "\n",
      "--- # of categories in variable 'Issue':  33\n",
      "\n",
      "--- 'Consumer complaint narrative' is either NUll or contains text of a complaint \n",
      "\n",
      "--- # of companies:  418\n",
      "\n",
      "--- # of U.S. states and territories:  62\n",
      "\n",
      "--- # of Tags:  3\n",
      "\n",
      "--- # of categories in 'Consumer consent provided?':  3\n",
      "\n",
      "--- # of Submission categories:  6\n",
      "\n",
      "--- Binary variable 'Timely_response'\n",
      "\n",
      "--- Varibale Year\n",
      "\n",
      "--- Variable Month\n",
      "\n",
      "--- Add constant term\n",
      "\n",
      "Done creating dummy variables\n",
      "______________________________________\n",
      "Done adding ACS (Census data)\n",
      "______________________________________\n",
      "Done dropping redundant columns\n",
      "______________________________________\n",
      "Size of transformed file: (85835, 555)\n"
     ]
    }
   ],
   "source": [
    "# Load the original CFPB data\n",
    "df_expanded =pd.read_csv(directory+'Consumer_Complaints_loaded_March20_2017.csv', header = 0, sep = ',', dtype = 'str')\n",
    "print('Size of original file: ', df_expanded.shape)\n",
    "print()\n",
    "\n",
    "# This function creates dummies and adds a constant term\n",
    "df_expanded = p.create_dummies_fn(df_expanded)\n",
    "\n",
    "# Load the ACS (Census) data\n",
    "socio_econ_data = pd.read_pickle(open(directory+'ACS_census_data.pickle', 'rb'))\n",
    "\n",
    "# This function adds ACS (Census) variables\n",
    "df_expanded = p.add_ACS_data_fn(df_expanded, socio_econ_data, ['ZIP code']+ACS_vars)\n",
    "\n",
    "# This function drops original CFPB variables (e.g., 'Issue', 'ZIP code', 'Company', etc.)\n",
    "df_expanded = p.drop_columns_for_logit(df_expanded)\n",
    "\n",
    "print('Size of transformed file:', df_expanded.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**df_expanded** contains the original categories of dummy variables, a constant term, and un-preprocessed ACS data. The number of rows in df_expanded (n=85,835) differs from the number of rows in df (n=84,575) due to the additional year 2011 observations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 More complete data: Unregularized logistic regression using preprocessed ACS variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of expanded train, valid, and test files:  (51501, 555) (17167, 555) (17167, 555)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create train, validation, and test sets \n",
    "train_expanded = df_expanded.sample(frac=0.6,replace = False, random_state=200)\n",
    "temp_expanded = df_expanded.drop(train_expanded.index)\n",
    "valid_expanded = temp_expanded.sample(frac=0.5,replace = False, random_state=100)\n",
    "test_expanded = temp_expanded.drop(valid_expanded.index)\n",
    "print(\"Size of expanded train, valid, and test files: \", train_expanded.shape, \\\n",
    "      valid_expanded.shape, test_expanded.shape)\n",
    "print()   \n",
    "\n",
    "# Calculate the scaler = means and standard deviations of ACS var in train expanded data\n",
    "scaler_expanded = preprocessing.StandardScaler().fit(train_expanded[ACS_vars])\n",
    "\n",
    "#Preprocess train, valid, and test data:\n",
    "train_expanded_preprocessed = train_expanded.copy()\n",
    "valid_expanded_preprocessed = valid_expanded.copy()\n",
    "test_expanded_preprocessed = test_expanded.copy()\n",
    "\n",
    "for i, var in enumerate(ACS_vars):\n",
    "\n",
    "    train_expanded_preprocessed[var] =\\\n",
    "    (train_expanded_preprocessed[var] - scaler_expanded.mean_[i])/scaler_expanded.scale_[i]  \n",
    "    \n",
    "    valid_expanded_preprocessed[var] =\\\n",
    "    (valid_expanded_preprocessed[var] - scaler_expanded.mean_[i])/scaler_expanded.scale_[i]  \n",
    "    \n",
    "    test_expanded_preprocessed[var] =\\\n",
    "    (test_expanded_preprocessed[var] - scaler_expanded.mean_[i])/scaler_expanded.scale_[i]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit unregularized logistic regression to preprocessed expanded train data using sklearn's package\n",
    "logit_expanded_unr = LogisticRegression(C=1e30, fit_intercept = False, solver='liblinear')\n",
    "logit_expanded_unr.fit(train_expanded_preprocessed.drop(['Y', 'Median_household_income'], axis=1), \\\n",
    "                       train_expanded_preprocessed['Y'])\n",
    "\n",
    "#initiate new dictionary for auc & log-loss values (for expanded dataset)\n",
    "auc_logloss_expanded_results = {}\n",
    "#store the original baseline auc & log-loss (the best performer on the df set)\n",
    "auc_logloss_expanded_results['base_no_preprocess'] = auc_and_logloss_results['base_no_preprocess'] \n",
    "\n",
    "# Calculate AUC (on valid) and log-loss (on train & valid)\n",
    "auc_logloss_expanded_results = \\\n",
    "        f.store_AUC_and_logloss_results(logit_expanded_unr, \\\n",
    "        'base_expanded_unr_preprocessed', True, \\\n",
    "        train_expanded_preprocessed, valid_expanded_preprocessed, \\\n",
    "        ['Y', 'Median_household_income'],auc_logloss_expanded_results)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_2_preprocess, threshold of 0.5': {'fpr': '0.021', 'fnr': '0.898'},\n",
       " 'base_expanded_unr_preprocessed': {'fpr': '0.026', 'fnr': '0.876'}}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate % of false positives and false negatives on the unregularized logit on the expanded set\n",
    "fpr_and_fnr_results =\\\n",
    "        f.store_fpr_and_fnr_results(logit_expanded_unr, \\\n",
    "         'base_expanded_unr_preprocessed', True,\n",
    "        valid_expanded_preprocessed, ['Y', 'Median_household_income'], 0.5, fpr_and_fnr_results)\n",
    "fpr_and_fnr_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# L2 regularized logit for several hyperparameter values on expanded, preprocessed data\n",
    "for i in range(6,-7, -1):\n",
    "    logreg = LogisticRegression(C=10**i, fit_intercept = False, solver='liblinear')\n",
    "    logreg.fit(train_expanded_preprocessed.drop(['Y', 'Median_household_income'], axis=1), \\\n",
    "               train_expanded_preprocessed['Y'])\n",
    "    \n",
    "    # Calculate AUC (on valid) and log-loss (on train & valid) \n",
    "    auc_logloss_expanded_results = \\\n",
    "        f.store_AUC_and_logloss_results(logreg, \\\n",
    "        'L2 C={} {}'.format(10**i, (7-len(str(10**i)))*\" \"), True, \\\n",
    "        train_expanded_preprocessed, valid_expanded_preprocessed, \\\n",
    "        ['Y', 'Median_household_income'],  auc_logloss_expanded_results)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUC</th>\n",
       "      <th>logloss_train</th>\n",
       "      <th>logloss_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>base_no_preprocess</th>\n",
       "      <td>0.713148</td>\n",
       "      <td>0.458000</td>\n",
       "      <td>0.467460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_expanded_unr_preprocessed</th>\n",
       "      <td>0.740367</td>\n",
       "      <td>0.444814</td>\n",
       "      <td>0.451924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=1000000</th>\n",
       "      <td>0.740382</td>\n",
       "      <td>0.444820</td>\n",
       "      <td>0.451817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=100000</th>\n",
       "      <td>0.740367</td>\n",
       "      <td>0.444815</td>\n",
       "      <td>0.451908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=10000</th>\n",
       "      <td>0.740378</td>\n",
       "      <td>0.444821</td>\n",
       "      <td>0.451801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=1000</th>\n",
       "      <td>0.740396</td>\n",
       "      <td>0.444833</td>\n",
       "      <td>0.451667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=100</th>\n",
       "      <td>0.740464</td>\n",
       "      <td>0.444916</td>\n",
       "      <td>0.451376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=10</th>\n",
       "      <td>0.740462</td>\n",
       "      <td>0.445266</td>\n",
       "      <td>0.451179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=1</th>\n",
       "      <td>0.740409</td>\n",
       "      <td>0.446108</td>\n",
       "      <td>0.451118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=0.1</th>\n",
       "      <td>0.738987</td>\n",
       "      <td>0.448293</td>\n",
       "      <td>0.452567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=0.01</th>\n",
       "      <td>0.726783</td>\n",
       "      <td>0.457620</td>\n",
       "      <td>0.461948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=0.001</th>\n",
       "      <td>0.687019</td>\n",
       "      <td>0.485041</td>\n",
       "      <td>0.489665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=0.0001</th>\n",
       "      <td>0.524474</td>\n",
       "      <td>0.530278</td>\n",
       "      <td>0.533798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=1e-05</th>\n",
       "      <td>0.475048</td>\n",
       "      <td>0.630065</td>\n",
       "      <td>0.631076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=1e-06</th>\n",
       "      <td>0.470590</td>\n",
       "      <td>0.684456</td>\n",
       "      <td>0.684583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     AUC logloss_train logloss_valid\n",
       "base_no_preprocess              0.713148      0.458000      0.467460\n",
       "base_expanded_unr_preprocessed  0.740367      0.444814      0.451924\n",
       "L2 C=1000000                    0.740382      0.444820      0.451817\n",
       "L2 C=100000                     0.740367      0.444815      0.451908\n",
       "L2 C=10000                      0.740378      0.444821      0.451801\n",
       "L2 C=1000                       0.740396      0.444833      0.451667\n",
       "L2 C=100                        0.740464      0.444916      0.451376\n",
       "L2 C=10                         0.740462      0.445266      0.451179\n",
       "L2 C=1                          0.740409      0.446108      0.451118\n",
       "L2 C=0.1                        0.738987      0.448293      0.452567\n",
       "L2 C=0.01                       0.726783      0.457620      0.461948\n",
       "L2 C=0.001                      0.687019      0.485041      0.489665\n",
       "L2 C=0.0001                     0.524474      0.530278      0.533798\n",
       "L2 C=1e-05                      0.475048      0.630065      0.631076\n",
       "L2 C=1e-06                      0.470590      0.684456      0.684583"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view_results_expanded = pd.DataFrame(auc_logloss_expanded_results).T\n",
    "view_results_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUC</th>\n",
       "      <th>logloss_train</th>\n",
       "      <th>logloss_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>L2 C=100</th>\n",
       "      <td>0.740464</td>\n",
       "      <td>0.444916</td>\n",
       "      <td>0.451376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    AUC logloss_train logloss_valid\n",
       "L2 C=100       0.740464      0.444916      0.451376"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the model that gives the highest AUC\n",
    "view_results_expanded[view_results_expanded.AUC == view_results_expanded.AUC.max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# L1 regularized logit for several hyperparameter values on expanded, preprocessed data\n",
    "for i in range(6,-7, -1):\n",
    "    logreg = LogisticRegression(penalty='l1', C=10**i, fit_intercept = False, solver='liblinear')\n",
    "    logreg.fit(train_expanded_preprocessed.drop(['Y', 'Median_household_income'], axis=1), \\\n",
    "               train_expanded_preprocessed['Y'])\n",
    "    \n",
    "    # Calculate AUC (on valid) and log-loss (on train & valid) \n",
    "    auc_logloss_expanded_results = \\\n",
    "        f.store_AUC_and_logloss_results(logreg, \\\n",
    "        'L1 C0={} {}'.format(10**i, (7-len(str(10**i)))*\" \"), True, \\\n",
    "        train_expanded_preprocessed, valid_expanded_preprocessed, \\\n",
    "        ['Y', 'Median_household_income'],  auc_logloss_expanded_results)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUC</th>\n",
       "      <th>logloss_train</th>\n",
       "      <th>logloss_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>base_no_preprocess</th>\n",
       "      <td>0.713148</td>\n",
       "      <td>0.458000</td>\n",
       "      <td>0.467460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>base_expanded_unr_preprocessed</th>\n",
       "      <td>0.740367</td>\n",
       "      <td>0.444814</td>\n",
       "      <td>0.451924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=1000000</th>\n",
       "      <td>0.740382</td>\n",
       "      <td>0.444820</td>\n",
       "      <td>0.451817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=100000</th>\n",
       "      <td>0.740367</td>\n",
       "      <td>0.444815</td>\n",
       "      <td>0.451908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=10000</th>\n",
       "      <td>0.740378</td>\n",
       "      <td>0.444821</td>\n",
       "      <td>0.451801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=1000</th>\n",
       "      <td>0.740396</td>\n",
       "      <td>0.444833</td>\n",
       "      <td>0.451667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=100</th>\n",
       "      <td>0.740464</td>\n",
       "      <td>0.444916</td>\n",
       "      <td>0.451376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=10</th>\n",
       "      <td>0.740462</td>\n",
       "      <td>0.445266</td>\n",
       "      <td>0.451179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=1</th>\n",
       "      <td>0.740409</td>\n",
       "      <td>0.446108</td>\n",
       "      <td>0.451118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=0.1</th>\n",
       "      <td>0.738987</td>\n",
       "      <td>0.448293</td>\n",
       "      <td>0.452567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=0.01</th>\n",
       "      <td>0.726783</td>\n",
       "      <td>0.457620</td>\n",
       "      <td>0.461948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=0.001</th>\n",
       "      <td>0.687019</td>\n",
       "      <td>0.485041</td>\n",
       "      <td>0.489665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=0.0001</th>\n",
       "      <td>0.524474</td>\n",
       "      <td>0.530278</td>\n",
       "      <td>0.533798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=1e-05</th>\n",
       "      <td>0.475048</td>\n",
       "      <td>0.630065</td>\n",
       "      <td>0.631076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L2 C=1e-06</th>\n",
       "      <td>0.470590</td>\n",
       "      <td>0.684456</td>\n",
       "      <td>0.684583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L1 C0=1000000</th>\n",
       "      <td>0.740324</td>\n",
       "      <td>0.444804</td>\n",
       "      <td>0.453692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L1 C0=100000</th>\n",
       "      <td>0.740325</td>\n",
       "      <td>0.444804</td>\n",
       "      <td>0.453156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L1 C0=10000</th>\n",
       "      <td>0.740327</td>\n",
       "      <td>0.444805</td>\n",
       "      <td>0.452621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L1 C0=1000</th>\n",
       "      <td>0.740338</td>\n",
       "      <td>0.444809</td>\n",
       "      <td>0.452086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L1 C0=100</th>\n",
       "      <td>0.740481</td>\n",
       "      <td>0.444853</td>\n",
       "      <td>0.451581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L1 C0=10</th>\n",
       "      <td>0.740327</td>\n",
       "      <td>0.445281</td>\n",
       "      <td>0.451376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L1 C0=1</th>\n",
       "      <td>0.740286</td>\n",
       "      <td>0.446345</td>\n",
       "      <td>0.451103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L1 C0=0.1</th>\n",
       "      <td>0.738293</td>\n",
       "      <td>0.449629</td>\n",
       "      <td>0.452836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L1 C0=0.01</th>\n",
       "      <td>0.712663</td>\n",
       "      <td>0.465628</td>\n",
       "      <td>0.469000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L1 C0=0.001</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.510828</td>\n",
       "      <td>0.515650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L1 C0=0.0001</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.595497</td>\n",
       "      <td>0.597071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L1 C0=1e-05</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L1 C0=1e-06</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     AUC logloss_train logloss_valid\n",
       "base_no_preprocess              0.713148      0.458000      0.467460\n",
       "base_expanded_unr_preprocessed  0.740367      0.444814      0.451924\n",
       "L2 C=1000000                    0.740382      0.444820      0.451817\n",
       "L2 C=100000                     0.740367      0.444815      0.451908\n",
       "L2 C=10000                      0.740378      0.444821      0.451801\n",
       "L2 C=1000                       0.740396      0.444833      0.451667\n",
       "L2 C=100                        0.740464      0.444916      0.451376\n",
       "L2 C=10                         0.740462      0.445266      0.451179\n",
       "L2 C=1                          0.740409      0.446108      0.451118\n",
       "L2 C=0.1                        0.738987      0.448293      0.452567\n",
       "L2 C=0.01                       0.726783      0.457620      0.461948\n",
       "L2 C=0.001                      0.687019      0.485041      0.489665\n",
       "L2 C=0.0001                     0.524474      0.530278      0.533798\n",
       "L2 C=1e-05                      0.475048      0.630065      0.631076\n",
       "L2 C=1e-06                      0.470590      0.684456      0.684583\n",
       "L1 C0=1000000                   0.740324      0.444804      0.453692\n",
       "L1 C0=100000                    0.740325      0.444804      0.453156\n",
       "L1 C0=10000                     0.740327      0.444805      0.452621\n",
       "L1 C0=1000                      0.740338      0.444809      0.452086\n",
       "L1 C0=100                       0.740481      0.444853      0.451581\n",
       "L1 C0=10                        0.740327      0.445281      0.451376\n",
       "L1 C0=1                         0.740286      0.446345      0.451103\n",
       "L1 C0=0.1                       0.738293      0.449629      0.452836\n",
       "L1 C0=0.01                      0.712663      0.465628      0.469000\n",
       "L1 C0=0.001                     0.500000      0.510828      0.515650\n",
       "L1 C0=0.0001                    0.500000      0.595497      0.597071\n",
       "L1 C0=1e-05                     0.500000      0.693147      0.693147\n",
       "L1 C0=1e-06                     0.500000      0.693147      0.693147"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view_results_expanded = pd.DataFrame(auc_logloss_expanded_results).T\n",
    "view_results_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUC</th>\n",
       "      <th>logloss_train</th>\n",
       "      <th>logloss_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>L1 C0=100</th>\n",
       "      <td>0.740481</td>\n",
       "      <td>0.444853</td>\n",
       "      <td>0.451581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     AUC logloss_train logloss_valid\n",
       "L1 C0=100       0.740481      0.444853      0.451581"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the model that gives the highest AUC\n",
    "view_results_expanded[view_results_expanded.AUC == view_results_expanded.AUC.max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_2_preprocess, threshold of 0.5': {'fpr': '0.021', 'fnr': '0.898'},\n",
       " 'base_expanded_unr_preprocessed': {'fpr': '0.026', 'fnr': '0.876'},\n",
       " 'L1_C100_expanded_preprocessed': {'fpr': '0.026', 'fnr': '0.876'}}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate % of false positives and false negatives for L1 with C=100 on the expanded dataset\n",
    "logreg_L1_C100 = LogisticRegression(penalty='l1', C=100, fit_intercept = False, solver='liblinear')\n",
    "logreg_L1_C100.fit(train_expanded_preprocessed.drop(['Y', 'Median_household_income'], axis=1), \\\n",
    "               train_expanded_preprocessed['Y'])\n",
    "\n",
    "fpr_and_fnr_results = \\\n",
    "    f.store_fpr_and_fnr_results(logreg_L1_C100, \\\n",
    "    'L1_C100_expanded_preprocessed', True, \\\n",
    "    valid_expanded_preprocessed, ['Y', 'Median_household_income'], 0.5, fpr_and_fnr_results)\n",
    "fpr_and_fnr_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results of using all available categories/labels in categorical variables:**\n",
    "\n",
    "Overall, using a complete set of labels in categorical variables makes models more nuanced, increases AUC, and lowers log-loss (at least for as long as regularization is not too strong). The best performer (as measured by AUC) on the expanded dataset is L1 regularized model with hyperparamer C=100.\n",
    "\n",
    "- AUC on original dataset with groupped categories (unregularized, best perfomer on dataset with groupped categories): 0.713148\n",
    "- AUC on expanded dataset (unregularized): 0.740367\n",
    "- AUC on expanded dataset (best performer on expanded dataset, L1 with C=100): 0.74048\n",
    "\n",
    "\n",
    "- Log-loss on original dataset with groupped categories (unregularized, best perfomer on dataset with groupped categories): 0.458000 (on train)\n",
    "- Log-loss on expanded dataset (unregularized): 0.444814\n",
    "- Log-loss on expanded dataset (best performer on expanded dataset, L1 with C=100): 0.444853\n",
    "\n",
    "\n",
    "- **Additional, smaller categories that made it into the model this time added some valuable information as even the unregularized model's (most prone to overfitting due to noise) AUC went up from 0.7131 to 0.7404 and log-loss slightly declined.** \n",
    "- **However, these effects seem small, and the value-added to the predictive power of our model is very marginal. Note that FNR or Pr(Type II error)=87.6% is still pretty high on the expanded set that uses all categories and regularization (went down by only 2 percentage points for the decision threshold of 0.5!).** \n",
    "- **Noise and overfitting do not seem to be a big issue as the unregularized model performs very similarly to the best perfomer L1 with C=100 (on the expanded set). In addition, log-loss on train and validation datasets are very close, what indicates that the models are neither overfit nor underfit.** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Balanced outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One other issue that I explore is the unbalanced nature of the outcome variable. Even though the case here is not as extreme as 1:99, for example, the ratio of positive to negative outcomes is roughly 20:80, i.e., still quite unbalanced. This means that the model favors predicting the negative outcome, and we have very high Pr(Type II error). One could perform downsampling or upsampling to arrive at a more balanced distribution of labels in the outcome. I choose a simpler, more practical way to examine this issue by utilizing 'class_weight' option in the sklearn's logistic regression package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of negative and positive outcomes: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    0.792637\n",
       "1    0.207363\n",
       "Name: Y, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('% of negative and positive outcomes: ')\n",
    "df_expanded.Y.value_counts()/df_expanded.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# L2 regularized logit with balanced class weight for several hyperparameter values on expanded, preprocessed data\n",
    "auc_logloss_balanced_results = {}\n",
    "for i in range(6,-7, -1):\n",
    "    logreg = LogisticRegression(C=10**i, fit_intercept = False, class_weight='balanced', \\\n",
    "            solver='liblinear')\n",
    "    logreg.fit(train_expanded_preprocessed.drop(['Y', 'Median_household_income'], axis=1), \\\n",
    "            train_expanded_preprocessed['Y'])\n",
    "    \n",
    "    # Calculate AUC (on valid) and log-loss (on train & valid) \n",
    "    auc_logloss_balanced_results = \\\n",
    "        f.store_AUC_and_logloss_results(logreg, \\\n",
    "        'L2 C={} {}'.format(10**i, (7-len(str(10**i)))*\" \"), True, \\\n",
    "        train_expanded_preprocessed, valid_expanded_preprocessed, \\\n",
    "        ['Y', 'Median_household_income'],  auc_logloss_balanced_results)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# L1 regularized logit with balanced class weight for several hyperparameter values on expanded, preprocessed data\n",
    "for i in range(6,-7, -1):\n",
    "    logreg = LogisticRegression(penalty='l1',C=10**i, fit_intercept=False, \\\n",
    "                                class_weight='balanced', solver='liblinear' )\n",
    "    logreg.fit(train_expanded_preprocessed.drop(['Y', 'Median_household_income'], axis=1), \\\n",
    "               train_expanded_preprocessed['Y'])\n",
    "    \n",
    "    # Calculate AUC (on valid) and log-loss (on train & valid) \n",
    "    auc_logloss_balanced_results = \\\n",
    "        f.store_AUC_and_logloss_results(logreg, \\\n",
    "        'L1 C0={} {}'.format(10**i, (7-len(str(10**i)))*\" \"), True, \\\n",
    "        train_expanded_preprocessed, valid_expanded_preprocessed, \\\n",
    "        ['Y', 'Median_household_income'],  auc_logloss_balanced_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUC</th>\n",
       "      <th>logloss_train</th>\n",
       "      <th>logloss_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>L2 C=1</th>\n",
       "      <td>0.740580</td>\n",
       "      <td>0.599034</td>\n",
       "      <td>0.599486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    AUC logloss_train logloss_valid\n",
       "L2 C=1         0.740580      0.599034      0.599486"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the model that gives the highest AUC\n",
    "view_results_balanced = pd.DataFrame(auc_logloss_balanced_results).T\n",
    "view_results_balanced[view_results_balanced.AUC == view_results_balanced.AUC.max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_2_preprocess, threshold of 0.5': {'fpr': '0.021', 'fnr': '0.898'},\n",
       " 'base_expanded_unr_preprocessed': {'fpr': '0.026', 'fnr': '0.876'},\n",
       " 'L1_C100_expanded_preprocessed': {'fpr': '0.026', 'fnr': '0.876'},\n",
       " 'L2_C1_expanded_balanced_w': {'fpr': '0.340', 'fnr': '0.317'}}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate % of false positives and false negatives for L2 with C=1 on the \n",
    "# expanded dataset with balanced weights\n",
    "logreg_L2_C1 = LogisticRegression(penalty='l2', C=1, fit_intercept = False, \\\n",
    "                                  class_weight='balanced', solver='liblinear')\n",
    "logreg_L2_C1.fit(train_expanded_preprocessed.drop(['Y', 'Median_household_income'], axis=1), \\\n",
    "               train_expanded_preprocessed['Y'])\n",
    "    \n",
    "fpr_and_fnr_results = \\\n",
    "    f.store_fpr_and_fnr_results(logreg_L2_C1, \\\n",
    "    'L2_C1_expanded_balanced_w', True, \\\n",
    "    valid_expanded_preprocessed, ['Y', 'Median_household_income'], 0.5, fpr_and_fnr_results)\n",
    "fpr_and_fnr_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Histograms of Predicted Probabilities when Y=1 and Y=1 using validation set\n",
    "Y_and_Y_hat_balanced = f.actual_and_predicted_values(logreg_L2_C1, \\\n",
    "                        valid_expanded_preprocessed, True, \\\n",
    "                        ['Y', 'Median_household_income'])\n",
    "f.predicted_proba_histograms_by_Y(Y_and_Y_hat_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Balanced weights, threshold of 0.6': {'fpr': '0.186', 'fnr': '0.498'},\n",
       " 'Balanced weights, threshold of 0.55': {'fpr': '0.257', 'fnr': '0.400'},\n",
       " 'Balanced weights, threshold of 0.5': {'fpr': '0.340', 'fnr': '0.317'},\n",
       " 'Balanced weights, threshold of 0.45': {'fpr': '0.428', 'fnr': '0.228'},\n",
       " 'Balanced weights, threshold of 0.4': {'fpr': '0.517', 'fnr': '0.158'},\n",
       " 'Balanced weights, threshold of 0.35': {'fpr': '0.604', 'fnr': '0.107'},\n",
       " 'Balanced weights, threshold of 0.3': {'fpr': '0.696', 'fnr': '0.074'},\n",
       " 'Balanced weights, threshold of 0.25': {'fpr': '0.784', 'fnr': '0.041'},\n",
       " 'Balanced weights, threshold of 0.2': {'fpr': '0.853', 'fnr': '0.018'}}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate % of false positives and false negatives on the 'balanced weights best performer'\n",
    "fpr_and_fnr_results_balanced = {}\n",
    "for i in  [i/100 for i in range(60,15,-5)]:\n",
    "    fpr_and_fnr_results_balanced = \\\n",
    "    f.store_fpr_and_fnr_results(logreg_L2_C1, \\\n",
    "    'Balanced weights, threshold of {}'.format(i), True, valid_expanded_preprocessed, \\\n",
    "    ['Y', 'Median_household_income'], i, fpr_and_fnr_results_balanced)\n",
    "fpr_and_fnr_results_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_no_preprocess, threshold of 0.6': {'fpr': '0.008', 'fnr': '0.943'},\n",
       " 'base_no_preprocess, threshold of 0.55': {'fpr': '0.012', 'fnr': '0.927'},\n",
       " 'base_no_preprocess, threshold of 0.5': {'fpr': '0.021', 'fnr': '0.898'},\n",
       " 'base_no_preprocess, threshold of 0.45': {'fpr': '0.041', 'fnr': '0.836'},\n",
       " 'base_no_preprocess, threshold of 0.4': {'fpr': '0.070', 'fnr': '0.743'},\n",
       " 'base_no_preprocess, threshold of 0.35': {'fpr': '0.103', 'fnr': '0.661'},\n",
       " 'base_no_preprocess, threshold of 0.3': {'fpr': '0.150', 'fnr': '0.590'},\n",
       " 'base_no_preprocess, threshold of 0.25': {'fpr': '0.237', 'fnr': '0.473'},\n",
       " 'base_no_preprocess, threshold of 0.2': {'fpr': '0.366', 'fnr': '0.327'}}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print baseline fpr and fnr results from above\n",
    "fpr_and_fnr_results_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUC on expanded dataset (unregularized): 0.740367\n",
    "AUC on expanded dataset (best performer on expanded dataset, L1 with C=100): 0.74048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results of using balanced weights:**\n",
    "\n",
    "- With balanced class weights, the L2 model with hyperparameter C=1 gives the highest AUC of 0.740580. This is a relatively minor improvement upon the L1 regularized model with C=100 and \"as is\" unbalanced class weights, where AUC is 0.74048. \n",
    "\n",
    "\n",
    "- The best performer with balanced weights has a much higher training-set log-loss of 0.599034. It is even higher than the training-set log-loss in the first, unregularized model on the dataset with groupped categories (0.458000). This is likely due to the higher weights on the minority class added to the loss function.\n",
    "\n",
    "\n",
    "- Histograms of predicted probabilities with balanced weights reveal a more desirable right-side skewness when Y=1 => a lower false negative rate at the majority of threshold values as more true positives are now correctly classified as positives.\n",
    "\n",
    "\n",
    "- However, for Y=0 the histogram of predicted probabilities is now more centered rather than left-skewed as before => more true negatives are now also identified as positives => higher false positive rate. \n",
    "\n",
    "\n",
    "- So, what do balanced weights achieve? A slightly higher AUC and a stronger bias towards the positive class than before but with a trade-off: the model is now predicting more positive outcomes, and we have a lower false negative rate (when Y=1) but a higher false positive rate (when Y=0). The model starts favoring the majority (negative) class less and starts predicting more of positive outcomes. As a result, for the given decision threshold of 0.5, the FNR falls from 0.898 to 0.317 (because we have more predicted positive labels now) but FPR increases from 0.021 to 0.340. Note that in an unbalanced model, we achieve a similar (FPR, FNR) = (0.366, 0.327) trade-off with a decision threshold of 0.2 that gives us more positive predictions than the decision threshold of 0.5.\n",
    "\n",
    "\n",
    "Next, instead of using zip code-level information captured in ACS (Census) prerocessed variables, I use zip code dummies instead. Remember that zip code dummies capture more or less permanent differences across zip codes (e.g., size, zip code's \"typical\" socio-economic ranking relative to other zip codes, etc.). \n",
    "\n",
    "Note that there are over 15,000 zip codes in the CFPB dataset. Some of the zip codes have been partially masked (e.g., the last two digits were replaced by 'XX'). So, I take 3-digit zip codes instead of 5-digit ones and include their dummies in the regression. The hope is that more granular geographic information could potentially give additional levers to correctly separate positive and negative outcomes. \n",
    "\n",
    "Because the model with balanced weights gave me the highest AUC so far, all models with zip codes will be trained with balanced weights as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Including Zip Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- # of categories in variable 'Issue':  33\n",
      "\n",
      "--- 'Consumer complaint narrative' is either NUll or contains text of a complaint \n",
      "\n",
      "--- # of companies:  418\n",
      "\n",
      "--- # of U.S. states and territories:  62\n",
      "\n",
      "--- # of Tags:  3\n",
      "\n",
      "--- # of categories in 'Consumer consent provided?':  3\n",
      "\n",
      "--- # of Submission categories:  6\n",
      "\n",
      "--- Binary variable 'Timely_response'\n",
      "\n",
      "--- Varibale Year\n",
      "\n",
      "--- Variable Month\n",
      "\n",
      "--- Add constant term\n",
      "\n",
      "Done creating dummy variables\n",
      "______________________________________\n",
      "# of 3-digit zip codes:  939\n",
      "\n",
      "Done creating 3-digit ZIP code dummies\n",
      "______________________________________\n",
      "Done dropping redundant columns\n",
      "______________________________________\n",
      "Size of transformed file with zip code dummies: (85835, 1482)\n"
     ]
    }
   ],
   "source": [
    "# Load the original CFPB data\n",
    "df_zip =pd.read_csv(directory+'Consumer_Complaints_loaded_March20_2017.csv', header = 0, sep = ',', dtype = 'str')\n",
    "\n",
    "# This function creates non-zip code dummies and adds a constant term\n",
    "df_zip = p.create_dummies_fn(df_zip)\n",
    "\n",
    "# This function creates zip code dummies\n",
    "df_zip = p.zip_code_dummies(df_zip)\n",
    "\n",
    "# This function drops original CFPB variables (e.g., 'Issue', 'ZIP code', 'Company', etc.)\n",
    "df_zip= p.drop_columns_for_logit(df_zip)\n",
    "\n",
    "print('Size of transformed file with zip code dummies:', df_zip.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of zip train, valid, and test files:  (51501, 1482) (17167, 1482) (17167, 1482)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create train, validation, and test sets \n",
    "train_zip = df_zip.sample(frac=0.6,replace = False, random_state=200)\n",
    "temp_zip = df_zip.drop(train_zip.index)\n",
    "valid_zip = temp_zip.sample(frac=0.5,replace = False, random_state=100)\n",
    "test_zip = temp_zip.drop(valid_zip.index)\n",
    "print(\"Size of zip train, valid, and test files: \", train_zip.shape, \\\n",
    "      valid_zip.shape, test_zip.shape)\n",
    "print()   \n",
    "\n",
    "# No preprocessing is necessary as all features are dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "auc_logloss_zip_results = {}\n",
    "# L2 regularized logit for several hyperparameter values on expanded data with \n",
    "# 3-digit zip code dummies\n",
    "for i in range(6,-7, -1):\n",
    "    logreg = LogisticRegression(C=10**i, fit_intercept = False, \\\n",
    "             class_weight='balanced', solver='liblinear')\n",
    "    logreg.fit(train_zip.drop(['Y'], axis=1), train_zip['Y'])\n",
    "    \n",
    "    # Calculate AUC (on valid) and log-loss (on train & valid) \n",
    "    auc_logloss_zip_results = \\\n",
    "        f.store_AUC_and_logloss_results(logreg, \\\n",
    "        'L2 C={} {}'.format(10**i, (7-len(str(10**i)))*\" \"), True, \\\n",
    "        train_zip, valid_zip, ['Y'],  auc_logloss_zip_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# L1 regularized logit with balanced class weight for several hyperparameter values on expanded data \n",
    "# with 3-digit zip code dummies\n",
    "for i in range(6,-7, -1):\n",
    "    logreg = LogisticRegression(penalty='l1',C=10**i, fit_intercept = False, \\\n",
    "             class_weight='balanced', solver='liblinear')\n",
    "    logreg.fit(train_zip.drop(['Y'], axis=1), train_zip['Y'])\n",
    "    \n",
    "    # Calculate AUC (on valid) and log-loss (on train & valid) \n",
    "    auc_logloss_zip_results = \\\n",
    "        f.store_AUC_and_logloss_results(logreg, \\\n",
    "        'L1 C0={} {}'.format(10**i, (7-len(str(10**i)))*\" \"), True, \\\n",
    "        train_zip, valid_zip, ['Y'],  auc_logloss_zip_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUC</th>\n",
       "      <th>logloss_train</th>\n",
       "      <th>logloss_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>L1 C0=0.1</th>\n",
       "      <td>0.740039</td>\n",
       "      <td>0.602398</td>\n",
       "      <td>0.600703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     AUC logloss_train logloss_valid\n",
       "L1 C0=0.1       0.740039      0.602398      0.600703"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the model that gives the highest AUC\n",
    "view_results_zip = pd.DataFrame(auc_logloss_zip_results).T\n",
    "view_results_zip[view_results_zip.AUC == view_results_zip.AUC.max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_2_preprocess, threshold of 0.5': {'fpr': '0.021', 'fnr': '0.898'},\n",
       " 'base_expanded_unr_preprocessed': {'fpr': '0.026', 'fnr': '0.876'},\n",
       " 'L1_C100_expanded_preprocessed': {'fpr': '0.026', 'fnr': '0.876'},\n",
       " 'L2_C1_expanded_balanced_w': {'fpr': '0.340', 'fnr': '0.317'},\n",
       " 'L1_C01_zip_balanced_w': {'fpr': '0.336', 'fnr': '0.318'}}"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate % of false positives and false negatives for L1 with C=0.1 on \n",
    "# the dataset with 3-digit zip codes\n",
    "logreg_L1_C01 = LogisticRegression(penalty='l1', C=0.1, fit_intercept = False, \\\n",
    "                class_weight='balanced', solver='liblinear')\n",
    "logreg_L1_C01.fit(train_zip.drop(['Y'], axis=1), train_zip['Y'])\n",
    "    \n",
    "fpr_and_fnr_results = \\\n",
    "    f.store_fpr_and_fnr_results(logreg_L1_C01, 'L1_C01_zip_balanced_w', True, \\\n",
    "    valid_zip, ['Y'], 0.5, fpr_and_fnr_results)\n",
    "fpr_and_fnr_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Issue dummies with zero coefficient:  4\n",
      "# of Company dummies with zero coefficient:  390\n",
      "# of State dummies with zero coefficient:  40\n",
      "# of Zip code dummies with zero coefficient:  903\n"
     ]
    }
   ],
   "source": [
    "# Count the number of dummies with estimates zero coefficients (L1 / Lasso regularization)\n",
    "f.count_zero_coefficients(logreg_L1_C01, valid_zip, [\"Y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Issue_Late fee</th>\n",
       "      <td>1.961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Issue_Other fee</th>\n",
       "      <td>1.701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Issue_Cash advance fee</th>\n",
       "      <td>1.316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Issue_Billing disputes</th>\n",
       "      <td>1.094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Issue_Balance transfer fee</th>\n",
       "      <td>0.999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Issue_Balance transfer</th>\n",
       "      <td>0.988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Company_Citibank</th>\n",
       "      <td>0.966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Issue_Overlimit fee</th>\n",
       "      <td>0.940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Issue_Transaction issue</th>\n",
       "      <td>0.891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Company_Barclays PLC</th>\n",
       "      <td>0.871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Company_Synchrony Financial</th>\n",
       "      <td>0.837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Issue_Credit card protection / Debt protection</th>\n",
       "      <td>0.830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Issue_Billing statement</th>\n",
       "      <td>0.794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Issue_APR or interest rate</th>\n",
       "      <td>0.763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Issue_Convenience checks</th>\n",
       "      <td>0.690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Issue_Payoff process</th>\n",
       "      <td>0.622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Issue_Cash advance</th>\n",
       "      <td>0.602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Company_USAA Savings</th>\n",
       "      <td>0.582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Company_Alliance Data Card Services</th>\n",
       "      <td>0.561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Company_TD Bank US Holding Company</th>\n",
       "      <td>0.478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Year_2013</th>\n",
       "      <td>0.384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Company_PayPal Holdings, Inc.</th>\n",
       "      <td>0.372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Issue_Identity theft / Fraud / Embezzlement</th>\n",
       "      <td>0.367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Company_Regions Financial Corporation</th>\n",
       "      <td>0.366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Company_Discover</th>\n",
       "      <td>0.337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Submitted_Web</th>\n",
       "      <td>0.318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Company_Capital One</th>\n",
       "      <td>0.305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Company_Comerica</th>\n",
       "      <td>0.305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Issue_Rewards</th>\n",
       "      <td>0.282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZIP_152</th>\n",
       "      <td>0.262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Company_JPMorgan Chase &amp; Co.</th>\n",
       "      <td>-0.085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Issue_Advertising and marketing</th>\n",
       "      <td>-0.090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tags_Servicemember</th>\n",
       "      <td>-0.096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Month_03</th>\n",
       "      <td>-0.098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZIP_334</th>\n",
       "      <td>-0.110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State_NV</th>\n",
       "      <td>-0.129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZIP_117</th>\n",
       "      <td>-0.158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Month_02</th>\n",
       "      <td>-0.161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZIP_925</th>\n",
       "      <td>-0.163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Consent_Consent not provided</th>\n",
       "      <td>-0.174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State_MS</th>\n",
       "      <td>-0.177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Timely_response</th>\n",
       "      <td>-0.232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Company_Portfolio Recovery Associates, Inc.</th>\n",
       "      <td>-0.240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZIP_078</th>\n",
       "      <td>-0.246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Issue_Unsolicited issuance of credit card</th>\n",
       "      <td>-0.264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Company_PNC Bank N.A.</th>\n",
       "      <td>-0.299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Company_BB&amp;T Financial</th>\n",
       "      <td>-0.348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Company_Navy FCU</th>\n",
       "      <td>-0.348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Company_HSBC North America Holdings Inc.</th>\n",
       "      <td>-0.412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Consent_Other</th>\n",
       "      <td>-0.437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Issue_Credit reporting</th>\n",
       "      <td>-0.575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Company_Continental Finance Company, LLC</th>\n",
       "      <td>-0.712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Issue_Credit line increase/decrease</th>\n",
       "      <td>-0.825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Issue_Application processing delay</th>\n",
       "      <td>-0.919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Issue_Bankruptcy</th>\n",
       "      <td>-0.935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Issue_Privacy</th>\n",
       "      <td>-1.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Company_Encore Capital Group</th>\n",
       "      <td>-1.098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>const</th>\n",
       "      <td>-1.408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Issue_Credit determination</th>\n",
       "      <td>-1.826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Year_2011</th>\n",
       "      <td>-3.577</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>141 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0\n",
       "Issue_Late fee                                  1.961\n",
       "Issue_Other fee                                 1.701\n",
       "Issue_Cash advance fee                          1.316\n",
       "Issue_Billing disputes                          1.094\n",
       "Issue_Balance transfer fee                      0.999\n",
       "Issue_Balance transfer                          0.988\n",
       "Company_Citibank                                0.966\n",
       "Issue_Overlimit fee                             0.940\n",
       "Issue_Transaction issue                         0.891\n",
       "Company_Barclays PLC                            0.871\n",
       "Company_Synchrony Financial                     0.837\n",
       "Issue_Credit card protection / Debt protection  0.830\n",
       "Issue_Billing statement                         0.794\n",
       "Issue_APR or interest rate                      0.763\n",
       "Issue_Convenience checks                        0.690\n",
       "Issue_Payoff process                            0.622\n",
       "Issue_Cash advance                              0.602\n",
       "Company_USAA Savings                            0.582\n",
       "Company_Alliance Data Card Services             0.561\n",
       "Company_TD Bank US Holding Company              0.478\n",
       "Year_2013                                       0.384\n",
       "Company_PayPal Holdings, Inc.                   0.372\n",
       "Issue_Identity theft / Fraud / Embezzlement     0.367\n",
       "Company_Regions Financial Corporation           0.366\n",
       "Company_Discover                                0.337\n",
       "Submitted_Web                                   0.318\n",
       "Company_Capital One                             0.305\n",
       "Company_Comerica                                0.305\n",
       "Issue_Rewards                                   0.282\n",
       "ZIP_152                                         0.262\n",
       "...                                               ...\n",
       "Company_JPMorgan Chase & Co.                   -0.085\n",
       "Issue_Advertising and marketing                -0.090\n",
       "Tags_Servicemember                             -0.096\n",
       "Month_03                                       -0.098\n",
       "ZIP_334                                        -0.110\n",
       "State_NV                                       -0.129\n",
       "ZIP_117                                        -0.158\n",
       "Month_02                                       -0.161\n",
       "ZIP_925                                        -0.163\n",
       "Consent_Consent not provided                   -0.174\n",
       "State_MS                                       -0.177\n",
       "Timely_response                                -0.232\n",
       "Company_Portfolio Recovery Associates, Inc.    -0.240\n",
       "ZIP_078                                        -0.246\n",
       "Issue_Unsolicited issuance of credit card      -0.264\n",
       "Company_PNC Bank N.A.                          -0.299\n",
       "Company_BB&T Financial                         -0.348\n",
       "Company_Navy FCU                               -0.348\n",
       "Company_HSBC North America Holdings Inc.       -0.412\n",
       "Consent_Other                                  -0.437\n",
       "Issue_Credit reporting                         -0.575\n",
       "Company_Continental Finance Company, LLC       -0.712\n",
       "Issue_Credit line increase/decrease            -0.825\n",
       "Issue_Application processing delay             -0.919\n",
       "Issue_Bankruptcy                               -0.935\n",
       "Issue_Privacy                                  -1.001\n",
       "Company_Encore Capital Group                   -1.098\n",
       "const                                          -1.408\n",
       "Issue_Credit determination                     -1.826\n",
       "Year_2011                                      -3.577\n",
       "\n",
       "[141 rows x 1 columns]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print estimated non-zero coefficients in descending order\n",
    "f.display_nonzero_coefficients(logreg_L1_C01,valid_zip, ['Y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results of including zip codes weights:**\n",
    "\n",
    "So now I can compare 3 models:\n",
    "\n",
    "- (1) The best performer (L1 with C=100) on the expanded dataset with preprocessed ACS variables and unbalanced weights\n",
    "- (2) The best performer (L2 with C=1) on the expanded dataset with preprocessed ACS variables and balanced weights\n",
    "- (3) The best performer (L1 with C=0.1) on the expanded dataset with 3-digit zip codes and balanced weights\n",
    "\n",
    "\\#3 (with zip codes and balanced weights) is essentially the same as \\#2 (expanded set with ACS variables and balanced weights), with slightly worse performance: AUC is slightly lower:  0.740039 < 0.740580. And log-loss on train data is slightly higher: 0.602398 > 0.599034.\n",
    "\n",
    "Compared to \\#1, \\#2 is doing better in terms of slightly higher AUC: 0.740580 > 0.740481 but log-loss on the train set is also higher 0.599034 > 0.444853 likely because the weights are increasing the log-loss.\n",
    "\n",
    "Note that the majority of zip code dummy variables were dropped as a result of L1 regularization. Out of 999 zip code dummies, 903 were dropped by the model. In addition, out of 62 U.S. state and territories, coefficients on 40 state dummies were set to zero. 390 Company dummies were estimated with a coefficient of zero (out of 418).\n",
    "\n",
    "**Overall, despite my attempt to obtain more information from a richer set of categories related to State, Zip code, and Company, L1 regularization led me back to a simpler model with a set of predictors similar to those in my original model with category grouppings: for example, the top predictors of monetary relief include dummies related to Late fee, Other fee, Cash advance fee, Billing disputes, Citibank, Synchrony Financial, Barclays, etc.**\n",
    "\n",
    "**Perhaps that is why the performace of logistic models with more nuanced data has not been significantly better: the main predictive signals are well captured in the simpler model, and additional data related to Company, State, and Zip codes likely does not help to better differentiate between positive and negative classes.**\n",
    "\n",
    "**On the other hand, out of 32 included Issue dumies, only 4 were dropped, and some appear to be among 'top' predictors (at least by the coefficient size) (e.g., Cash advance fee). Hence, the inclusion of additional Issue categories seems to have contributed to improving model performance (from the baseline).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Decision tree modeling for comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I fit a decision tree to the expanded data with preprocessed ACS variables - first, an unregularized tree and then a regularized one by constraining the tree's maximum depth. The purpose of this exercise is to compare the performance of a logistic regression vs a decision tree model. \n",
    "\n",
    "Remember that the performance of unregularized and regularized logit models above was not very different. \n",
    "- One reason for this could be due to data and limited signals in data that can help us differentiate between positive and negatives outcomes.\n",
    "\n",
    "\n",
    "- Another reason could be that the train and validation datasets are very similar. Under a different split of data, the train and validation sets could be sufficiently different and give rise to more overfitting.\n",
    "\n",
    "\n",
    "- Third, unregularized logistic regression is probably less prone to overfitting than unregularized decision trees that can fit training data perfectly. Hence, there should be a larger differential in AUCs between unregularized and regularized decision tree models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit an unregularized decision tree on the expanded dataset with 3-digit zip code dummies\n",
    "clf_base = DecisionTreeClassifier(criterion='entropy')\n",
    "clf_base = clf_base.fit(train_expanded_preprocessed.drop(['Y', 'Median_household_income'], \\\n",
    "         axis=1), train_expanded_preprocessed['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base': 0.5823647750537216}"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate AUC\n",
    "y_score_base = clf_base.predict_proba(valid_expanded_preprocessed.drop(['Y', \\\n",
    "                'Median_household_income'], axis=1))[:,1]\n",
    "auc_dt = {}\n",
    "auc_dt['base'] = roc_auc_score(valid_expanded_preprocessed['Y'], y_score_base)\n",
    "auc_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base': 0.5823647750537216,\n",
       " 'DT 5': 0.6631387391855477,\n",
       " 'DT 10': 0.6898505806229348,\n",
       " 'DT 15': 0.6656326558872102,\n",
       " 'DT 20': 0.6363239607638325,\n",
       " 'DT 25': 0.5979549036458011,\n",
       " 'DT 30': 0.5866328349559474,\n",
       " 'DT 35': 0.574702773027703,\n",
       " 'DT 40': 0.5658215307696516,\n",
       " 'DT 45': 0.5739172308311078,\n",
       " 'DT 50': 0.5698761280075868}"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit decision trees of various depths. Use validation set to identify the depth \n",
    "# that gives the highest AUC\n",
    "for i in range(5,51, 5):\n",
    "    clf = DecisionTreeClassifier(criterion='entropy', max_depth = i)\n",
    "    clf = clf.fit(train_expanded_preprocessed.drop(['Y', 'Median_household_income'], \\\n",
    "            axis=1), train_expanded_preprocessed['Y'])\n",
    "    y_score = \\\n",
    "        clf.predict_proba(valid_expanded_preprocessed.drop(['Y', 'Median_household_income'],\\\n",
    "        axis=1))[:,1]\n",
    "    auc_dt['DT {}'.format(i)] = roc_auc_score(valid_expanded_preprocessed.Y, y_score)\n",
    "auc_dt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DT 10'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the model that gives the highest AUC\n",
    "max(auc_dt,key=auc_dt.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the differential in AUCs is much higher when I use decision tree models. In the unregularized decision tree, AUC is 0.582365. It is much lower than the AUCs in more regularized decision trees with depths of 5, 10, 15, and 20. The AUC in unregularized DT is also much lower than in unregularized logit, likely because of the perfect fit to training data (0.582365 vs 0.713148).\n",
    "\n",
    "The decision tree of depth 10 has the highest AUC of 0.689851. Again, this AUC is below the AUC of 0.740580 of the best logistic model trained on the expanded dataset with preprocessed ACS variables and balanaced weights. This could indicate that the logistic regression is a more suitable model to represent the relationship between predictors and the outcome.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Histograms of predicted probabilities when Y=1 and Y=1 using validation set\n",
    "# (unregularized decision tree)\n",
    "Y_and_Y_hat_dt_unr = f.actual_and_predicted_values(clf_base, \\\n",
    "                        valid_expanded_preprocessed, True, \\\n",
    "                        ['Y', 'Median_household_income'])\n",
    "f.predicted_proba_histograms_by_Y(Y_and_Y_hat_dt_unr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit a decision tree of depth 10.\n",
    "clf_d10 = DecisionTreeClassifier(criterion='entropy', max_depth = 10)\n",
    "clf_d10 = clf_d10.fit(train_expanded_preprocessed.drop(['Y', 'Median_household_income'], \\\n",
    "         axis=1), train_expanded_preprocessed['Y'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot histograms of predicted probabilities for Y=1 and Y=0 (decision tree of depth 10)\n",
    "Y_and_Y_hat_dt_d10 = f.actual_and_predicted_values(clf_d10, \\\n",
    "                        valid_expanded_preprocessed, True, \\\n",
    "                        ['Y', 'Median_household_income'])\n",
    "f.predicted_proba_histograms_by_Y(Y_and_Y_hat_dt_d10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_2_preprocess, threshold of 0.5': {'fpr': '0.021', 'fnr': '0.898'},\n",
       " 'base_expanded_unr_preprocessed': {'fpr': '0.026', 'fnr': '0.876'},\n",
       " 'L1_C100_expanded_preprocessed': {'fpr': '0.026', 'fnr': '0.876'},\n",
       " 'L2_C1_expanded_balanced_w': {'fpr': '0.340', 'fnr': '0.317'},\n",
       " 'L1_C01_zip_balanced_w': {'fpr': '0.336', 'fnr': '0.318'},\n",
       " 'DT unregularized': {'fpr': '0.181', 'fnr': '0.655'},\n",
       " 'DT of depth 10': {'fpr': '0.023', 'fnr': '0.886'}}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr_and_fnr_results = \\\n",
    "    f.store_fpr_and_fnr_results(clf_base, 'DT unregularized', True, \\\n",
    "    valid_expanded_preprocessed, ['Y', 'Median_household_income'], 0.5, fpr_and_fnr_results)\n",
    "\n",
    "fpr_and_fnr_results = \\\n",
    "    f.store_fpr_and_fnr_results(clf_d10, 'DT of depth 10', True, \\\n",
    "    valid_expanded_preprocessed, ['Y', 'Median_household_income'], 0.5, fpr_and_fnr_results)\n",
    "fpr_and_fnr_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "Judging by false positive and false negative rates, the unregularized DT tends to favor positive outcomes more than the regularized decision tree of depth 15. \n",
    "\n",
    "Overall, decision tree models are performing worse than the majority of logistic regression models tried above. If non-linearities do not play a significant role in predicting outcomes, and linear models are powerful enough to capture signals in data, then the subpar performance of decision trees relative to logistic regressions is not surprising.\n",
    "\n",
    "In the next IPython Notebook, I peform some additional data analyses and modeling, including taking a closer look at the data points where my logitic models make mistakes and trying models with interaction terms. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
